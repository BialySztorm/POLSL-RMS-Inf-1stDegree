{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a13bf749",
   "metadata": {},
   "source": [
    "# Laboratory 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90694d20",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "Create dummy input and target data as follows:\n",
    "```python\n",
    "import numpy as np\n",
    "title_data = np.random.randint(0, 2, size=(num_samples, vocabulary_size))\n",
    "text_body_data = np.random.randint(0, 2, size=(num_samples, vocabulary_size))\n",
    "tags_data = np.random.randint(0, 2, size=(num_samples, num_tags))\n",
    "\n",
    "priority_data = np.random.random(size=(num_samples, 1))\n",
    "department_data = np.random.randint(0, 2, size=(num_samples, num_departments))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc80196",
   "metadata": {},
   "source": [
    "- Next compile model with:\n",
    "    - `rmsprop` omptimizer, \n",
    "    - the `mean_squared_error` loss function for priority, the `categorical_crossentropy` loss function for department, \n",
    "    - the `mean_absolute_error` metric for priority and the `accuracy` metric for department.\n",
    "- Fit the model choosing the number of epochs=5.\n",
    "- Evaluate the model. What are the values of loss functions and metrics for target data?\n",
    "- Do predictions using the model. What department was predicted by the model?\n",
    "- What are the model layers?\n",
    "- Print inputs and outputs for all layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad9b1be",
   "metadata": {},
   "source": [
    "Add another output to the previous model — we want to estimate how long a given issue ticket will take to resolve. Do this via a classification layer over three categories: `quick`, `medium`, and `difficult`. Don’t recreate a model from scratch but start from the intermediate features of the previous model:\n",
    "```python\n",
    "features = model.layers[4].output                                            \n",
    "difficulty = layers.Dense(3, activation=\"softmax\", name=\"difficulty\")(features)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2e5b61e72c6edd",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1c0c9a79601aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "num_samples = 1000\n",
    "vocabulary_size = 100\n",
    "num_tags = 10\n",
    "num_departments = 5\n",
    "\n",
    "# Generate dummy input data\n",
    "title_data = np.random.randint(0, 2, size=(num_samples, vocabulary_size))\n",
    "text_body_data = np.random.randint(0, 2, size=(num_samples, vocabulary_size))\n",
    "tags_data = np.random.randint(0, 2, size=(num_samples, num_tags))\n",
    "\n",
    "# Generate dummy target data\n",
    "priority_data = np.random.random(size=(num_samples, 1))\n",
    "department_data = np.random.randint(0, 2, size=(num_samples, num_departments))\n",
    "\n",
    "# Define model inputs\n",
    "title_input = keras.Input(shape=(vocabulary_size,), name=\"title\")\n",
    "text_body_input = keras.Input(shape=(vocabulary_size,), name=\"text_body\")\n",
    "tags_input = keras.Input(shape=(num_tags,), name=\"tags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b557e74a9baf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all inputs\n",
    "x = layers.concatenate([title_input, text_body_input, tags_input])\n",
    "x = layers.Dense(64, activation=\"relu\")(x)\n",
    "x = layers.Dense(32, activation=\"relu\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fffc70a2e878d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model outputs\n",
    "priority_output = layers.Dense(1, name=\"priority\")(x)\n",
    "department_output = layers.Dense(num_departments, activation=\"softmax\", name=\"department\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277f6dfdc9f6c4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = keras.Model(\n",
    "    inputs=[title_input, text_body_input, tags_input],\n",
    "    outputs=[priority_output, department_output]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe9d554fd1182e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss={\"priority\": \"mean_squared_error\", \"department\": \"categorical_crossentropy\"},\n",
    "    metrics={\"priority\": \"mean_absolute_error\", \"department\": \"accuracy\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f44f6fcb2a83c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    {\"title\": title_data, \"text_body\": text_body_data, \"tags\": tags_data},\n",
    "    {\"priority\": priority_data, \"department\": department_data},\n",
    "    epochs=5,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d0830a2671cd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "results = model.evaluate(\n",
    "    {\"title\": title_data, \"text_body\": text_body_data, \"tags\": tags_data},\n",
    "    {\"priority\": priority_data, \"department\": department_data}\n",
    ")\n",
    "print(\"Evaluation results:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4a36d4086cf0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "preds = model.predict({\"title\": title_data, \"text_body\": text_body_data, \"tags\": tags_data})\n",
    "predicted_departments = np.argmax(preds[1], axis=1)\n",
    "print(\"Predicted departments:\", predicted_departments[:10])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "# Print input and output shapes for all layers\n",
    "for layer in model.layers:\n",
    "    input_shape = getattr(layer, \"input_shape\", getattr(layer, \"batch_input_shape\", \"N/A\"))\n",
    "    output_shape = getattr(layer, \"output_shape\", \"N/A\")\n",
    "    print(f\"Layer: {layer.name}, Input shape: {input_shape}, Output shape: {output_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4945c8bae58d0cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new output for existing model\n",
    "features = model.layers[4].output  # layer Dense(32)\n",
    "difficulty = layers.Dense(3, activation=\"softmax\", name=\"difficulty\")(features)\n",
    "\n",
    "model2 = keras.Model(\n",
    "    inputs=model.inputs,\n",
    "    outputs=[priority_output, department_output, difficulty]\n",
    ")\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a441a8ae",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "Implement  a custom metric that measures the mean absolute error (MAE). Next create model using `get_mnist_model()` and evaluate model using test data. What is the value of your MAE metrics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5973db8390b565",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee592a3c14c1c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_mae(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred_labels = tf.cast(tf.argmax(y_pred, axis=-1), tf.float32)\n",
    "    return tf.reduce_mean(tf.abs(y_true - y_pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6531714946e470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST data i model\n",
    "(train_images, train_labels), (test_images, test_labels) = keras.datasets.mnist.load_data()\n",
    "train_images = train_images.reshape((-1, 28*28)).astype(\"float32\") / 255\n",
    "test_images = test_images.reshape((-1, 28*28)).astype(\"float32\") / 255\n",
    "\n",
    "def get_mnist_model():\n",
    "    inputs = keras.Input(shape=(28*28,))\n",
    "    x = layers.Dense(64, activation=\"relu\")(inputs)\n",
    "    outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
    "    return keras.Model(inputs, outputs)\n",
    "\n",
    "model = get_mnist_model()\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[custom_mae])\n",
    "model.fit(train_images, train_labels, epochs=2, batch_size=64)\n",
    "results = model.evaluate(test_images, test_labels)\n",
    "print(\"Custom MAE:\", results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e349aa",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "Define function `scheduler(epoch, lr)` which for the number of epochs - `epoch` and a learning rate - `lr`, updates `lr` as follows:\n",
    "$lr\\cdot\\exp\\left(-\\frac{epoch}{10^2}\\right)$. Modify callbacks_list from the lecture by adding  `keras.callbacks.LearningRateScheduler(scheduler)` and monitoring only `val_loss`. Next create model using the `get_mnist_model()` function, compile and fit the model with same parameters as in the lecture. Compare evaluations of the model for validation and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5af7cc8bb46c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47d596835c381d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    return lr * math.exp(-epoch / 100)\n",
    "\n",
    "callback = keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "model = get_mnist_model()\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = model.fit(\n",
    "    train_images, train_labels,\n",
    "    epochs=5,\n",
    "    validation_data=(test_images, test_labels),\n",
    "    callbacks=[callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962e6daa",
   "metadata": {},
   "source": [
    "## Task 4\n",
    "Modify callback from the lecture in such a way that your callback will save a list of per-batch accuracy values during training and create a data frame of these values at the end of each epoch. Moreover, your callback should plot a graph of per-batch accuracy values for the first epoch at the end of the first epoch and plot a graph of per-batch accuracy values for all epochs aprart from the first epoch at the end of the training. For the followng code:\n",
    "```python\n",
    "model = get_mnist_model()\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit(train_images, train_labels,\n",
    "          epochs=5,\n",
    "          callbacks=[MetricsHistory()],\n",
    "          validation_data=(val_images, val_labels))\n",
    "```\n",
    "you should obtain the following results \n",
    "<img src=\"../lectures/Lecture 5-20250525/2.png\"/>\n",
    "<img src=\"../lectures/Lecture 5-20250525/3.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a6994385eb37e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf04462e1ce4a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.batch_accuracies = []\n",
    "        self.epoch_accuracies = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        self.batch_accuracies.append(logs.get(\"accuracy\"))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        df = pd.DataFrame({\"batch_accuracy\": self.batch_accuracies})\n",
    "        self.epoch_accuracies.append(self.batch_accuracies.copy())\n",
    "        if epoch == 0:\n",
    "            plt.figure()\n",
    "            plt.plot(self.batch_accuracies)\n",
    "            plt.title(\"Per-batch accuracy (epoch 1)\")\n",
    "            plt.show()\n",
    "        self.batch_accuracies = []\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if len(self.epoch_accuracies) > 1:\n",
    "            plt.figure()\n",
    "            for i, acc in enumerate(self.epoch_accuracies[1:]):\n",
    "                plt.plot(acc, label=f\"Epoch {i+2}\")\n",
    "            plt.title(\"Per-batch accuracy (kolejne epoki)\")\n",
    "            plt.legend()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4fc0161e2c5cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_mnist_model()\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.fit(train_images, train_labels, epochs=5, callbacks=[MetricsHistory()], validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5ac16d-012c-4ee2-b098-02c2b4fbf243",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
