{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdb26d84",
   "metadata": {},
   "source": [
    "# Laboratory 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7069b0c",
   "metadata": {},
   "source": [
    "## Description of the `IMDB` dataset\n",
    "The `IMDB` dataset is a set of 50,000 highly polarized reviews from the Internet Movie Database. Theyâ€™re split into 25,000 reviews for training and 25,000 reviews for testing, each set consisting of 50% negative and 50% positive reviews. The reviews (sequences of words) have been preprocessed - turned into sequences of integers, where each integer stands for a specific word in a dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc967e40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T12:19:54.708572Z",
     "iopub.status.busy": "2025-05-25T12:19:54.708190Z",
     "iopub.status.idle": "2025-05-25T12:20:04.629790Z",
     "shell.execute_reply": "2025-05-25T12:20:04.628783Z",
     "shell.execute_reply.started": "2025-05-25T12:19:54.708554Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(\n",
    "    num_words=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a80ff1f",
   "metadata": {},
   "source": [
    "The argument `num_words=10000` means weâ€™ll only keep the top 10,000 most frequently occurring words in the training data. Rare words will be discarded. The variables `train_data` and `test_data` are lists of reviews; each review is a list of word indices (encoding a sequence of words). `train_labels` and `test_labels` are lists of 0s and 1s, where 0 stands for negative and 1 stands for positive. For instance, the first review consists of 218 words and is positive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24df61f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T12:20:04.631920Z",
     "iopub.status.busy": "2025-05-25T12:20:04.631182Z",
     "iopub.status.idle": "2025-05-25T12:20:04.639849Z",
     "shell.execute_reply": "2025-05-25T12:20:04.639024Z",
     "shell.execute_reply.started": "2025-05-25T12:20:04.631868Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.array(train_data[0]),len(train_data[0]),train_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d27670",
   "metadata": {},
   "source": [
    "We can easily decode any of these reviews back to English words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933e57d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T12:20:04.660753Z",
     "iopub.status.busy": "2025-05-25T12:20:04.660496Z",
     "iopub.status.idle": "2025-05-25T12:20:04.779430Z",
     "shell.execute_reply": "2025-05-25T12:20:04.778730Z",
     "shell.execute_reply.started": "2025-05-25T12:20:04.660736Z"
    }
   },
   "outputs": [],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce777937",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T12:20:04.793943Z",
     "iopub.status.busy": "2025-05-25T12:20:04.793593Z",
     "iopub.status.idle": "2025-05-25T12:20:04.798085Z",
     "shell.execute_reply": "2025-05-25T12:20:04.797233Z",
     "shell.execute_reply.started": "2025-05-25T12:20:04.793898Z"
    }
   },
   "outputs": [],
   "source": [
    "def decoded_review(number_of_review):\n",
    "    return \" \".join([reverse_word_index.get(i - 3, \"?\") for i in train_data[number_of_review]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3509c982",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T12:20:04.818612Z",
     "iopub.status.busy": "2025-05-25T12:20:04.817957Z",
     "iopub.status.idle": "2025-05-25T12:20:04.824402Z",
     "shell.execute_reply": "2025-05-25T12:20:04.823717Z",
     "shell.execute_reply.started": "2025-05-25T12:20:04.818572Z"
    }
   },
   "outputs": [],
   "source": [
    "number_of_review = 1000\n",
    "decoded_review(number_of_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cb2b3a-4d4f-4618-86fb-cae246d1ac49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T12:20:04.842892Z",
     "iopub.status.busy": "2025-05-25T12:20:04.842468Z",
     "iopub.status.idle": "2025-05-25T12:20:05.721643Z",
     "shell.execute_reply": "2025-05-25T12:20:05.720873Z",
     "shell.execute_reply.started": "2025-05-25T12:20:04.842873Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers, optimizers\n",
    "from tensorflow.keras.datasets import imdb\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6289d2a8",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "Prepare the data:\n",
    "- Multi-hot encode lists from `train_data` and `train_labels` to turn them into vectors of 0s and 1s. This would mean, for instance, turning the sequence [8, 5] into a 10,000-dimensional vector that would be all 0s except for indices 8 and 5, which would be 1s. Then you could use a Dense layer, capable of handling floating-point vector data, as the first layer in your model.\n",
    "- Change data type in `test_data` and `test_labels` from `int64` into `float32`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a61aba-db14-46e3-b402-e97deef37345",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T12:20:05.742947Z",
     "iopub.status.busy": "2025-05-25T12:20:05.742652Z",
     "iopub.status.idle": "2025-05-25T12:20:09.886304Z",
     "shell.execute_reply": "2025-05-25T12:20:09.885566Z",
     "shell.execute_reply.started": "2025-05-25T12:20:05.742920Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the data (keep top 10,000 words only)\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e06666-b736-4060-b998-6eb8281a6cef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T12:20:09.903314Z",
     "iopub.status.busy": "2025-05-25T12:20:09.902720Z",
     "iopub.status.idle": "2025-05-25T12:20:11.694510Z",
     "shell.execute_reply": "2025-05-25T12:20:11.693885Z",
     "shell.execute_reply.started": "2025-05-25T12:20:09.903291Z"
    }
   },
   "outputs": [],
   "source": [
    "# Multi-hot encode the input sequences\n",
    "def multi_hot_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension), dtype=\"float32\")\n",
    "    for i, seq in enumerate(sequences):\n",
    "        results[i, seq] = 1.0\n",
    "    return results\n",
    "\n",
    "x_train = multi_hot_sequences(train_data)\n",
    "x_test = multi_hot_sequences(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eacd05a-1f44-4a34-ae11-da644e654ef9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T12:20:11.709397Z",
     "iopub.status.busy": "2025-05-25T12:20:11.708726Z",
     "iopub.status.idle": "2025-05-25T12:20:11.712599Z",
     "shell.execute_reply": "2025-05-25T12:20:11.711942Z",
     "shell.execute_reply.started": "2025-05-25T12:20:11.709357Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ensure labels are float32 (for compatibility)\n",
    "y_train = np.array(train_labels).astype(\"float32\")\n",
    "y_test = np.array(test_labels).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a919e12",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "Build your model. Take into consideration that the input data is vectors, and the labels are scalars (1s and 0s) and a type of model that performs well on such a problem is a plain stack of densely connected (Dense) layers with relu activations. Think about:\n",
    "- How many layers to use?\n",
    "- How many units to choose for each layer?\n",
    "\n",
    "Compile your model choosing a proper optimizer, loss function, and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fcc526-90be-4d60-9ea8-6b592b0b5fb4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T12:20:11.726513Z",
     "iopub.status.busy": "2025-05-25T12:20:11.726271Z",
     "iopub.status.idle": "2025-05-25T12:20:11.737637Z",
     "shell.execute_reply": "2025-05-25T12:20:11.736780Z",
     "shell.execute_reply.started": "2025-05-25T12:20:11.726496Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model(units=[16, 16], activation='relu', loss='binary_crossentropy'):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Input(shape=(10000,)))\n",
    "    for u in units:\n",
    "        model.add(layers.Dense(u, activation=activation))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=loss,\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983b40e5",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "Validate your model: \n",
    "- Create a validation set by setting apart 10,000 samples from the original training data.\n",
    "- Train the model for 20 epochs in mini-batches of 512 samples from training data.\n",
    "- Monitor loss and accuracy on the 10,000 samples from the validation set.\n",
    "- Make a plot of the training and validation loss.\n",
    "</br><img src=2.png/>\n",
    "- Make a plot of the training and validation accuracy. \n",
    "</br><img src=3.png/>\n",
    "- Choose a proper number of epochs to train the model on the entire train data to prevent overfitting, and then evaluate it on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7408425b-e4b1-4d40-9591-8349fef86d44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T12:20:11.751219Z",
     "iopub.status.busy": "2025-05-25T12:20:11.750963Z",
     "iopub.status.idle": "2025-05-25T12:20:11.761844Z",
     "shell.execute_reply": "2025-05-25T12:20:11.761153Z",
     "shell.execute_reply.started": "2025-05-25T12:20:11.751199Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a validation set\n",
    "x_val = x_train[:10000]\n",
    "partial_x_train = x_train[10000:]\n",
    "\n",
    "y_val = y_train[:10000]\n",
    "partial_y_train = y_train[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d5a943-be8f-4f2d-9c69-91e3ce8015b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T12:20:11.775858Z",
     "iopub.status.busy": "2025-05-25T12:20:11.775234Z",
     "iopub.status.idle": "2025-05-25T12:20:31.657037Z",
     "shell.execute_reply": "2025-05-25T12:20:31.656042Z",
     "shell.execute_reply.started": "2025-05-25T12:20:11.775830Z"
    }
   },
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "\n",
    "history = model.fit(\n",
    "    partial_x_train, partial_y_train,\n",
    "    epochs=20,\n",
    "    batch_size=512,\n",
    "    validation_data=(x_val, y_val),\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4bddad-6298-47a8-8e06-a48fc122bf45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T12:20:31.707751Z",
     "iopub.status.busy": "2025-05-25T12:20:31.705983Z",
     "iopub.status.idle": "2025-05-25T12:20:32.340355Z",
     "shell.execute_reply": "2025-05-25T12:20:32.339722Z",
     "shell.execute_reply.started": "2025-05-25T12:20:31.707721Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot training and validation loss\n",
    "def plot_history(history):\n",
    "    history_dict = history.history\n",
    "    acc = history_dict['accuracy']\n",
    "    val_acc = history_dict['val_accuracy']\n",
    "    loss = history_dict['loss']\n",
    "    val_loss = history_dict['val_loss']\n",
    "    epochs = range(1, len(loss) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b28f55f-258c-4969-a97e-00fe953b7f9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T12:20:32.357307Z",
     "iopub.status.busy": "2025-05-25T12:20:32.357023Z",
     "iopub.status.idle": "2025-05-25T12:20:38.887623Z",
     "shell.execute_reply": "2025-05-25T12:20:38.886894Z",
     "shell.execute_reply.started": "2025-05-25T12:20:32.357289Z"
    }
   },
   "outputs": [],
   "source": [
    "# Retrain final model using best epoch (e.g. 4)\n",
    "final_model = build_model()\n",
    "final_model.fit(x_train, y_train, epochs=4, batch_size=512, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74c80c8-3bbf-41fa-8137-ff94ade742f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T12:20:38.902265Z",
     "iopub.status.busy": "2025-05-25T12:20:38.901972Z",
     "iopub.status.idle": "2025-05-25T12:20:42.405844Z",
     "shell.execute_reply": "2025-05-25T12:20:42.404985Z",
     "shell.execute_reply.started": "2025-05-25T12:20:38.902246Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "results = final_model.evaluate(x_test, y_test, verbose=2)\n",
    "print(f\"\\nTest Loss: {results[0]:.4f}, Test Accuracy: {results[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a99212d",
   "metadata": {},
   "source": [
    "## Task 4\n",
    "Do the following experiments:\n",
    "- Try using less or more representation layers, and see how doing so affects validation and test accuracy.\n",
    "- Try using layers with more units or fewer units.\n",
    "- Try using the `mse` loss function instead of `binary_crossentropy`.\n",
    "- Try using the `tanh` activation instead of `relu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25afb30a-c1b8-45d5-adab-e64b6d0ad6ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T12:20:42.422097Z",
     "iopub.status.busy": "2025-05-25T12:20:42.421814Z",
     "iopub.status.idle": "2025-05-25T12:20:42.426445Z",
     "shell.execute_reply": "2025-05-25T12:20:42.425862Z",
     "shell.execute_reply.started": "2025-05-25T12:20:42.422078Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "import gc\n",
    "\n",
    "def experiment(description, **kwargs):\n",
    "    print(f\"\\n=== {description} ===\")\n",
    "    model = build_model(**kwargs)\n",
    "    model.fit(partial_x_train, partial_y_train,\n",
    "              epochs=4,\n",
    "              batch_size=512,\n",
    "              validation_data=(x_val, y_val),\n",
    "              verbose=0)\n",
    "    results = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print(f\"Test Accuracy: {results[1]:.4f}\")\n",
    "\n",
    "    # ðŸ§¹ Cleanup to avoid memory issues\n",
    "    K.clear_session()\n",
    "    del model\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a87b11-698f-449c-a995-9a18bf3110cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-25T12:20:42.442228Z",
     "iopub.status.busy": "2025-05-25T12:20:42.441881Z"
    }
   },
   "outputs": [],
   "source": [
    "# Experiment 1: Fewer layers\n",
    "experiment(\"1 layer with 16 units\", units=[16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f455b30-884d-480f-af79-1de3a2bf4bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2: More layers\n",
    "experiment(\"3 layers with 16 units\", units=[16, 16, 16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0b7871-7b32-4906-9ef3-0e635c46faa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 3: Larger layers\n",
    "experiment(\"2 layers with 64 units\", units=[64, 64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fea1be5-5c8a-4bf6-b008-1c76448442b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 4: Smaller layers\n",
    "experiment(\"2 layers with 4 units\", units=[4, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebab449b-100b-40b2-9377-247f0d0cec5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 5: MSE loss instead of binary_crossentropy\n",
    "experiment(\"MSE loss function\", loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ac826d-7f5a-4e78-aa00-50ae3f0e6183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 6: tanh instead of relu\n",
    "experiment(\"tanh activation\", activation='tanh')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-ai-2024.04-py310",
   "language": "python",
   "name": "conda-env-anaconda-ai-2024.04-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
