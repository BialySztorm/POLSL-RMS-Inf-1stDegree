{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc1f11fd",
   "metadata": {},
   "source": [
    "# Lecture 5 A - Different ways to create Keras models.\n",
    "There are three APIs for building models in Keras:\n",
    "- The **Sequential model**, the most approachable API—it’s basically a Python list. As such, it’s limited to simple stacks of layers.\n",
    "- The **Functional API**, which focuses on graph-like model architectures. It represents a nice mid-point between usability and flexibility, and as such, it’s the most commonly used model-building API.\n",
    "- **Model subclassing**, a low-level option where we write everything ourself from scratch. This is ideal if we want full control over every little thing. However, we won’t get access to many built-in Keras features, and we will be more at risk of making mistakes.\n",
    "<img src=1.png/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b39a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras \n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc39f8a",
   "metadata": {},
   "source": [
    "## The Sequential model\n",
    "is the simplest way to build a Keras model. \n",
    "```python\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(64, activation=\"relu\"),\n",
    "    layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "```\n",
    "It’s possible to build the same model incrementally via the add() method, which is equivalent of the `append()` method of a Python list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c86688b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(layers.Dense(64, activation=\"relu\"))\n",
    "model.add(layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd9e299",
   "metadata": {},
   "source": [
    "Layers only get built when they are called for the first time. That’s because the shape of the layers' weights depends on the shape of their input. The  Sequential model does not have any weights until we actually call it on some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ae4e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4694fbd",
   "metadata": {},
   "source": [
    "We can also call `build()` method with an input shape to make model have weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768532a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build(input_shape=(None, 3))  \n",
    "model.weights    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d59d03",
   "metadata": {},
   "source": [
    "After the model is built, we can display its contents via the `summary()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c505b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d1002d",
   "metadata": {},
   "source": [
    "We can give names to everything in Keras—every model, every layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7414ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(name=\"Example_model\")\n",
    "model.add(layers.Dense(64, activation=\"relu\", name=\"first_layer\"))\n",
    "model.add(layers.Dense(10, activation=\"softmax\", name=\"last_layer\"))\n",
    "model.build((None, 3))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09ac7ee",
   "metadata": {},
   "source": [
    "When building a Sequential model incrementally, it’s useful to be able to print a summary of what the current model looks like. But we can’t print a summary until the model is built. The solution is to declare the shape of the model’s inputs via the Input class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ae318c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.Input(shape=(3,)))               \n",
    "model.add(layers.Dense(64, activation=\"relu\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a962526d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.Dense(10, activation=\"softmax\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0203abd",
   "metadata": {},
   "source": [
    "## The Functional API\n",
    "The Sequential model is easy to use, but its applicability is extremely limited: it can only express models with a single input and a single output, applying one layer after the other in a sequential fashion. In practice, it’s pretty common to encounter models with multiple inputs (say, an image and its metadata), multiple outputs (different things you want to predict about the data), or a nonlinear topology. In such cases, you’d build your model using the Functional API.\n",
    "\n",
    "Functional API version of the previous model looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983fcc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(3,), name=\"my_input\")\n",
    "features = layers.Dense(64, activation=\"relu\")(inputs)\n",
    "outputs = layers.Dense(10, activation=\"softmax\")(features)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d13eb5",
   "metadata": {},
   "source": [
    "The inputs object holds information about the shape and dtype of the data that the model will process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4788fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2e07e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ef2232",
   "metadata": {},
   "source": [
    "Next, we creat a layer and called it on the input\n",
    "```python\n",
    "features = layers.Dense(64, activation=\"relu\")(inputs)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbfa1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b98fcf",
   "metadata": {},
   "source": [
    "After obtaining the final outputs, we instantiate the model by specifying its inputs and outputs in the Model constructor:\n",
    "```python\n",
    "outputs = layers.Dense(10, activation=\"softmax\")(features)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "```\n",
    "\n",
    "The summary of the model is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17c5665",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4878c5dd",
   "metadata": {},
   "source": [
    "The following model aplies possibilities of the functional API in broader range. \n",
    "Let us consider a system to rank customer support tickets by priority and route them to the appropriate department. Our model has three inputs:\n",
    "- The title of the ticket (text input)\n",
    "- The text body of the ticket (text input)\n",
    "- Any tags added by the user (categorical input, assumed here to be one-hot encoded)\n",
    "\n",
    "Our model has two outputs:\n",
    "- The priority score of the ticket, a scalar between 0 and 1 (sigmoid output)\n",
    "- The department that should handle the ticket (a softmax over the set of departments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042e5b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 10000\n",
    "num_tags = 100\n",
    "num_departments = 4\n",
    "\n",
    "title = keras.Input(shape=(vocabulary_size,), name=\"title\")\n",
    "text_body = keras.Input(shape=(vocabulary_size,), name=\"text_body\")\n",
    "tags = keras.Input(shape=(num_tags,), name=\"tags\")\n",
    "\n",
    "features = layers.Concatenate()([title, text_body, tags])\n",
    "features = layers.Dense(64, activation=\"relu\")(features)\n",
    "\n",
    "priority = layers.Dense(1, activation=\"sigmoid\", name=\"priority\")(features)\n",
    "department = layers.Dense(\n",
    "    num_departments, activation=\"softmax\", name=\"department\")(features)\n",
    "\n",
    "model = keras.Model(inputs=[title, text_body, tags], outputs=[priority, department])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffddb7d3",
   "metadata": {},
   "source": [
    "We can inspect and reuse individual layers in the model. The model.layers model property provides the list of layers that make up the model, and for each layer we can query layer.input and layer.output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4174a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c959815c",
   "metadata": {},
   "source": [
    "For instance we want to get input and output of layer[3]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f362c82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[3].input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe38aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[3].output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47f321d",
   "metadata": {},
   "source": [
    "This enables us to do feature extraction, creating models that reuse intermediate features from another model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2cc47e",
   "metadata": {},
   "source": [
    "## Model subclassing\n",
    "is the most advanced model-building pattern. \n",
    "Subclassing Model involves:\n",
    "- In the `__init__()` method, define the layers the model will use.\n",
    "- In the `call()` method, define the forward pass of the model, reusing the layers previously created.\n",
    "- Instantiate subclass, and call it on data to create its weights.\n",
    "\n",
    "The Model subclassing workflow is the most flexible way to build a model. It enables us to build models that cannot be expressed as directed acyclic graphs of layers — imagine, for instance, a model where the `call()` method uses layers inside a for loop, or calls them recursively.\n",
    "\n",
    "We will reimplement the customer support ticket management model using a Model subclass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ce90e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomerTicketModel(keras.Model):\n",
    "\n",
    "    def __init__(self, num_departments):\n",
    "        super().__init__() # Call the super() constructor\n",
    "        # Define sublayers in the constructor:\n",
    "        self.concat_layer = layers.Concatenate()\n",
    "        self.mixing_layer = layers.Dense(64, activation=\"relu\")\n",
    "        self.priority_scorer = layers.Dense(1, activation=\"sigmoid\")\n",
    "        self.department_classifier = layers.Dense(num_departments, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs): # Define the forward pass in the call() method\n",
    "        title = inputs[\"title\"]\n",
    "        text_body = inputs[\"text_body\"]\n",
    "        tags = inputs[\"tags\"]\n",
    "\n",
    "        features = self.concat_layer([title, text_body, tags])\n",
    "        features = self.mixing_layer(features)\n",
    "        priority = self.priority_scorer(features)\n",
    "        department = self.department_classifier(features)\n",
    "        return priority, department"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e50266",
   "metadata": {},
   "source": [
    "Once we’ve defined the model, we can instantiate it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85752099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_samples = 1280\n",
    "\n",
    "title_data = np.random.randint(0, 2, size=(num_samples, vocabulary_size))\n",
    "text_body_data = np.random.randint(0, 2, size=(num_samples, vocabulary_size))\n",
    "tags_data = np.random.randint(0, 2, size=(num_samples, num_tags))\n",
    "\n",
    "priority, department = model(\n",
    "    {\"title\": title_data, \"text_body\": text_body_data, \"tags\": tags_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6549f9",
   "metadata": {},
   "source": [
    "We can compile and train a Model subclass just like a Sequential or Functional model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab6ab2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "priority_data = np.random.random(size=(num_samples, 1))\n",
    "department_data = np.random.randint(0, 2, size=(num_samples, num_departments))\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              #  The structure of what we pass as the loss and metrics arguments must match exactly \n",
    "              # what gets returned by call()—here, a list of two elements:\n",
    "              loss=[\"mean_squared_error\", \"categorical_crossentropy\"],\n",
    "              metrics=[[\"mean_absolute_error\"], [\"accuracy\"]])\n",
    "model.fit({\"title\": title_data,\n",
    "           \"text_body\": text_body_data,\n",
    "           \"tags\": tags_data},\n",
    "          [priority_data, department_data],\n",
    "          epochs=1)\n",
    "model.evaluate(#The structure of the input data must match exactly what is expected by the call() method—\n",
    "               # here, a dict with keys title, text_body, and tags:\n",
    "                {\"title\": title_data,\n",
    "                \"text_body\": text_body_data,\n",
    "                \"tags\": tags_data},\n",
    "                # The structure of the target data must match exactly what is returned by the call() method—\n",
    "                # here, a list of two elements:\n",
    "               [priority_data, department_data])\n",
    "priority_preds, department_preds = model.predict({\"title\": title_data,\n",
    "                                                  \"text_body\": text_body_data,\n",
    "                                                  \"tags\": tags_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171fda10",
   "metadata": {},
   "source": [
    "Functional and subclassed models are substantially different in nature. A Functional model is an explicit data structure—a graph of layers, which we can view, inspect, and modify. A subclassed model is a piece of bytecode—a Python class with a `call()` method that contains raw code. This is the source of the subclassing workflow’s flexibility but it introduces new limitations:\n",
    "- the way layers are connected to each other is hidden inside the body of the call() method, we cannot access that information;\n",
    "- calling `summary()` will not display layer connectivity;\n",
    "- we cannot plot the model topology via plot_model();\n",
    "- given a subclassed model, we cannot access layers to do feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a48e26",
   "metadata": {},
   "source": [
    "## Mixing and matching different components\n",
    "Choosing one of these patterns—the Sequential model, the Functional API, or Model subclassing—does not lock us out of the others. All models in the Keras API can smoothly interoperate with each other.\n",
    "\n",
    "For instance, we can use a subclassed layer or model in a Functional model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106555f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(keras.Model):\n",
    "\n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__()\n",
    "        if num_classes == 2:\n",
    "            num_units = 1\n",
    "            activation = \"sigmoid\"\n",
    "        else:\n",
    "            num_units = num_classes\n",
    "            activation = \"softmax\"\n",
    "        self.dense = layers.Dense(num_units, activation=activation)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.dense(inputs)\n",
    "\n",
    "inputs = keras.Input(shape=(3,))\n",
    "features = layers.Dense(64, activation=\"relu\")(inputs)\n",
    "outputs = Classifier(num_classes=10)(features)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfd06d3",
   "metadata": {},
   "source": [
    "Inversely, we can use a Functional model as part of a subclassed layer or model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb6d14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(64,))\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(inputs)\n",
    "binary_classifier = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "class MyModel(keras.Model):\n",
    "\n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.dense = layers.Dense(64, activation=\"relu\")\n",
    "        self.classifier = binary_classifier\n",
    "\n",
    "    def call(self, inputs):\n",
    "        features = self.dense(inputs)\n",
    "        return self.classifier(features)\n",
    "\n",
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ce7f66",
   "metadata": {},
   "source": [
    "## What workflow should we choose for building Keras models?\n",
    "In general, the Functional API provides us with a pretty good trade-off between ease of use and flexibility. It also gives us direct access to layer connectivity, which is very powerful for use cases such as model plotting or feature extraction. If we can use the Functional API — that is, if our model can be expressed as a directed acyclic graph of layers — it is recommended using it over model subclassing.\n",
    "\n",
    "In general, using Functional models that include subclassed layers provides the best of both worlds: high development flexibility while retaining the advantages of the Functional API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d18db9",
   "metadata": {},
   "source": [
    "# Lecture 5B - using training and evaluation loops\n",
    "The principle of progressive disclosure of complexity also applies to model training. Keras provides us with different workflows for training models. They can be as simple as calling fit() on our data, or as advanced as writing a new training algorithm from scratch.\n",
    "\n",
    "The standard workflow encompasses: `compile()`, `fit()`, `evaluate()`, and `predict()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a986e4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "def get_mnist_model():\n",
    "    inputs = keras.Input(shape=(28 * 28,))\n",
    "    features = layers.Dense(512, activation=\"relu\")(inputs)\n",
    "    features = layers.Dropout(0.5)(features)\n",
    "    outputs = layers.Dense(10, activation=\"softmax\")(features)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "(images, labels), (test_images, test_labels) = mnist.load_data()\n",
    "images = images.reshape((60000, 28 * 28)).astype(\"float32\") / 255\n",
    "test_images = test_images.reshape((10000, 28 * 28)).astype(\"float32\") / 255\n",
    "train_images, val_images = images[10000:], images[:10000]\n",
    "train_labels, val_labels = labels[10000:], labels[:10000]\n",
    "\n",
    "model = get_mnist_model()\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit(train_images, train_labels,\n",
    "          epochs=3,\n",
    "          validation_data=(val_images, val_labels))\n",
    "test_metrics = model.evaluate(test_images, test_labels)\n",
    "predictions = model.predict(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a55334d",
   "metadata": {},
   "source": [
    "There are, however, a couple of ways we can customize this simple workflow:\n",
    "- Creating our own custom metrics.\n",
    "- Passing callbacks to the `fit()` method to schedule actions to be taken at specific points during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7750063",
   "metadata": {},
   "source": [
    "## Creating our own custom metrics\n",
    "Metrics are key to measuring the performance of our model — in particular, to measuring the difference between its performance on the training data and its performance on the test data. Commonly used metrics for classification and regression are already part of the built-in `keras.metrics` module. But if we’re doing anything out of the ordinary, we will need to be able to write our own metrics. A Keras metric is a subclass of the `keras.metrics.Metric` class. Like layers, a metric has an internal state stored in TensorFlow variables. Unlike layers, these variables aren’t updated via backpropagation, so we have to write the state-update logic ourself, which happens in the `update_state()` method.\n",
    "\n",
    "Below we implement an example of a custom metric that measures the root mean squared error (RMSE):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde52f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class RootMeanSquaredError(keras.metrics.Metric): # Subclass the Metric class\n",
    "\n",
    "    # Define the state variables in the constructor. Like for layers, we have access to the add_weight() method\n",
    "    def __init__(self, name=\"rmse\", **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.mse_sum = self.add_weight(name=\"mse_sum\", initializer=\"zeros\")\n",
    "        self.total_samples = self.add_weight(name=\"total_samples\", initializer=\"zeros\", dtype=\"int32\")       \n",
    "    \n",
    "    # Implement the state update logic in update_state(). The y_true argument is the targets (or labels) for one batch, \n",
    "    # while y_pred represents the corresponding predictions from the model. \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.one_hot(y_true, depth=tf.shape(y_pred)[1]) # To match our MNIST model, we expect categorical predictions and integer labels.\n",
    "        mse = tf.reduce_sum(tf.square(y_true - y_pred))\n",
    "        self.mse_sum.assign_add(mse)\n",
    "        num_samples = tf.shape(y_pred)[0]\n",
    "        self.total_samples.assign_add(num_samples)\n",
    "    \n",
    "    # Return the current value of the metric\n",
    "    def result(self):\n",
    "        return tf.sqrt(self.mse_sum / tf.cast(self.total_samples, tf.float32))\n",
    "\n",
    "    # Reset the metric state without having to reinstantiate it\n",
    "    def reset_state(self):\n",
    "        self.mse_sum.assign(0.)\n",
    "        self.total_samples.assign(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3d7eca",
   "metadata": {},
   "source": [
    "Our custom metrics can be used like built-in ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f20aeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_mnist_model()\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\", RootMeanSquaredError()])\n",
    "model.fit(train_images, train_labels,\n",
    "          epochs=3,\n",
    "          validation_data=(val_images, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8888e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = model.evaluate(test_images, test_labels)\n",
    "test_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dbde5b",
   "metadata": {},
   "source": [
    "### Using callbacks\n",
    "A **callback** is an object (a class instance implementing specific methods) that is passed to the model in the call to `fit()` and that is called by the model at various points during training. It has access to all the available data about the state of the model and its performance, and it can take action: interrupt training, save a model, load a different weight set, or otherwise alter the state of the model.\n",
    "\n",
    "We can use callbacks in the following ways:\n",
    "- Model checkpointing—Saving the current state of the model at different points during training.\n",
    "- Early stopping—Interrupting training when the validation loss is no longer improving.\n",
    "- Dynamically adjusting the value of certain parameters during training—Such as the learning rate of the optimizer.\n",
    "- Logging training and validation metrics during training, or visualizing the representations learned by the model as they’re updated.\n",
    "\n",
    "The `keras.callbacks` module includes a number of built-in callbacks:\n",
    "- `keras.callbacks.ModelCheckpoint`\n",
    "- `keras.callbacks.EarlyStopping`\n",
    "- `keras.callbacks.LearningRateScheduler`\n",
    "- `keras.callbacks.ReduceLROnPlateau`\n",
    "- `keras.callbacks.CSVLogger`\n",
    "\n",
    "When we’re training a model, we can’t tell how many epochs will be needed to get to an optimal validation loss. We have adopted the strategy of training for enough epochs that we begin overfitting, using the first run to figure out the proper number of epochs to train for, and then finally launching a new training run from scratch using this optimal number. However, this approach is wasteful. A much better way to handle this is to stop training when we measure that the validation loss is no longer improving. This can be achieved using the *EarlyStopping* callback. The *EarlyStopping* callback interrupts training once a target metric being monitored has stopped improving for a fixed number of epochs. This callback is typically used in combination with *ModelCheckpoint*, which lets us continually save the model during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa983578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks are passed to the model via the callbacks argument in fit(), which takes a list of callbacks. \n",
    "callbacks_list = [\n",
    "    keras.callbacks.EarlyStopping(# Interrupts training when improvement stops\n",
    "        monitor=\"val_accuracy\",# Monitors the model’s validation accuracy\n",
    "        patience=2,# Interrupts training when accuracy has stopped improving for 2 epochs\n",
    "    ),\n",
    "    keras.callbacks.ModelCheckpoint(# Saves the current weights after every epoch\n",
    "        filepath=\"checkpoint_path.keras\",# Path to the destination model file\n",
    "        monitor=\"val_loss\",# val_loss is monitored\n",
    "        save_best_only=True,# Only the best model seen during training will be kept\n",
    "    )\n",
    "]\n",
    "model = get_mnist_model()\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit(train_images, train_labels,\n",
    "          epochs=15,\n",
    "          callbacks=callbacks_list,\n",
    "          validation_data=(val_images, val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede02988",
   "metadata": {},
   "source": [
    "We can easily save models manually after training:\n",
    "```python\n",
    "model.save('my_checkpoint_path')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42b0ec1",
   "metadata": {},
   "source": [
    "To reload the model we’ve saved, we use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9f9423",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"checkpoint_path.keras\")\n",
    "print(model.evaluate(val_images, val_labels))\n",
    "model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d100034",
   "metadata": {},
   "source": [
    "### Writing our own callbacks\n",
    "If we need to take a specific action during training that isn’t covered by one of the built-in callbacks, we can write our own callback. Callbacks are implemented by subclassing the `keras.callbacks.Callback` class. We can then implement any number of the following transparently named methods, which are called at various points during training:\n",
    "```python\n",
    "on_epoch_begin(epoch, logs) # Called at the start of every epoch     \n",
    "on_epoch_end(epoch, logs)   # Called at the end of every epoch     \n",
    "on_batch_begin(batch, logs) # Called before processing each batch     \n",
    "on_batch_end(batch, logs)   # Called after processing each batch        \n",
    "on_train_begin(logs)        # Called at the start of training     \n",
    "on_train_end(logs)          # Called at the end of training  \n",
    "```\n",
    "\n",
    "All these methods are called with a logs argument, which is a dictionary containing information about the previous batch, epoch, or training run — training and validation metrics, and so on.\n",
    "\n",
    "The following callback saves a list of per-batch loss values during training and saves a graph of these values at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4479f810",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs):\n",
    "        self.per_batch_losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        self.per_batch_losses.append(logs.get(\"loss\"))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        plt.clf()\n",
    "        plt.plot(range(len(self.per_batch_losses)), self.per_batch_losses,\n",
    "                 label=\"Training loss for each batch\")\n",
    "        plt.xlabel(f\"Batch (epoch {epoch})\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.savefig(f\"Plots/plot_at_epoch_{epoch}\")\n",
    "        self.per_batch_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43243621",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_mnist_model()\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit(train_images, train_labels,\n",
    "          epochs=10,\n",
    "          callbacks=[LossHistory()],\n",
    "          validation_data=(val_images, val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcb1ba6",
   "metadata": {},
   "source": [
    "### Monitoring and visualization with TensorBoard\n",
    "To do good research or develop good models, we need rich, frequent feedback about what’s going on inside our models during our experiments. That’s the point of running experiments: to get information about how well a model performs. **TensorBoard** (www.tensorflow.org/tensorboard) is a browser-based application that we can run locally. It’s the best way to monitor everything that goes on inside our model during training. With TensorBoard, we can:\n",
    "- Visually monitor metrics during training\n",
    "- Visualize our model architecture\n",
    "- Visualize histograms of activations and gradients\n",
    "- Explore embeddings in 3D\n",
    "\n",
    "The easiest way to use TensorBoard with a Keras model and the `fit()` method is to use the `keras.callbacks.TensorBoard` callback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce14c0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_mnist_model()\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "tensorboard = keras.callbacks.TensorBoard(\n",
    "    #log_dir=\"/full_path_to_your_log_dir\",\n",
    "    log_dir=\"D:/Dokumenty/log_dir\",\n",
    ")\n",
    "model.fit(train_images, train_labels,\n",
    "          epochs=5,\n",
    "          validation_data=(val_images, val_labels),\n",
    "          callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7824ef",
   "metadata": {},
   "source": [
    "Once the model starts running, it will write logs at the target location. We can run an embedded TensorBoard instance as part of your notebook, using the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36d673c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir \"D:/Dokumenty/log_dir\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4cb7ff",
   "metadata": {},
   "source": [
    "## Writing our own training and evaluation loops\n",
    "The built-in `fit()` workflow is solely focused on supervised learning: a setup where there are known targets associated with input data, and where we compute our loss as a function of these targets and the model’s predictions. There are other setups where no explicit targets are present, such as generative learning, self-supervised learning (where targets are obtained from the inputs), and reinforcement learning (where learning is driven by occasional “rewards,”). In such situations the built-in `fit()` is not enough, and we will need to write our own custom training logic. \n",
    "\n",
    "The contents of a typical training loop look like this:\n",
    "- Run the forward pass (compute the model’s output) inside a gradient tape to obtain a loss value for the current batch of data.\n",
    "- Retrieve the gradients of the loss with regard to the model’s weights.\n",
    "- Update the model’s weights so as to lower the loss value on the current batch of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674eac34",
   "metadata": {},
   "source": [
    "To reimplement `fit()` from scratch we need to take into consideration the following facts:\n",
    "1. Some Keras layers, such as the Dropout layer, have different behaviors during training and during inference (when we use them to generate predictions). Such layers expose a training Boolean argument in their `call()` method. Calling `dropout(inputs, training=True)` will drop some activation entries, while calling `dropout(inputs, training=False)` does nothing. So we need to pass training =True when we call a Keras model during the forward pass. Our forward pass thus becomes `predictions = model(inputs, training=True)`.\n",
    "2. When we retrieve the gradients of the weights of our model, we should not use `tape.gradients(loss, model.weights)`, but rather `tape .gradients(loss, model.trainable_weights)` since layers and models own two kinds of weights:\n",
    "- Trainable weights — These are meant to be updated via backpropagation to minimize the loss of the model.\n",
    "- Non-trainable weights — These are meant to be updated during the forward pass by the layers that own them.\n",
    "\n",
    "Taking into account these two facts, a supervised-learning training step ends up looking like this:\n",
    "```python\n",
    "def train_step(inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs, training=True)\n",
    "        loss = loss_fn(targets, predictions)\n",
    "    gradients = tape.gradients(loss, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(model.trainable_weights, gradients))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226b86e1",
   "metadata": {},
   "source": [
    "3. In a low-level training loop, we can leverage Keras metrics. As we know the metrics API consits of: simply call `update_state(y_true, y_pred)` for each batch of targets and predictions, and then use `result()` to query the current metric value. For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bff3c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = range(10)\n",
    "mean_tracker = keras.metrics.Mean() \n",
    "for value in values:\n",
    "    mean_tracker.update_state(value) \n",
    "mean_tracker.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eb9d79",
   "metadata": {},
   "source": [
    "The following training step function combines the forward pass, backward pass, and metrics tracking into a fit()-like function  that takes a batch of data and targets and returns the logs that would get displayed by the `fit()` progress bar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d0207d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_mnist_model()\n",
    "\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = keras.optimizers.RMSprop()\n",
    "metrics = [keras.metrics.SparseCategoricalAccuracy()]\n",
    "loss_tracking_metric = keras.metrics.Mean()\n",
    "\n",
    "@tf.function\n",
    "def train_step(inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs, training=True)\n",
    "        loss = loss_fn(targets, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "\n",
    "    logs = {}\n",
    "    for metric in metrics:\n",
    "        metric.update_state(targets, predictions)\n",
    "        logs[metric.name] = metric.result()\n",
    "\n",
    "    loss_tracking_metric.update_state(loss)\n",
    "    logs[\"loss\"] = loss_tracking_metric.result()\n",
    "    return logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d63724",
   "metadata": {},
   "source": [
    "4. We also need to use `metric.reset_state()` when we want to reset the current results (at the start of a training epoch or at the start of evaluation). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779176c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_metrics():\n",
    "    for metric in metrics:\n",
    "        metric.reset_state()\n",
    "    loss_tracking_metric.reset_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b19647f",
   "metadata": {},
   "source": [
    "Now we can lay out our complete training loop. We use a `tf.data.Dataset` object to turn our NumPy data into an iterator that iterates over the data in batches of size 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955808cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "training_dataset = training_dataset.batch(32)\n",
    "epochs = 2\n",
    "for epoch in range(epochs):\n",
    "    reset_metrics()\n",
    "    for inputs_batch, targets_batch in training_dataset:\n",
    "        logs = train_step(inputs_batch, targets_batch)\n",
    "    print(f\"Results at the end of epoch {epoch}\")\n",
    "    for key, value in logs.items():\n",
    "        print(f\"...{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4279508",
   "metadata": {},
   "source": [
    "The evaluation loop is a simple `for` loop that repeatedly calls a `test_step()` function, which processes a single batch of data. The `test_step()` function is a subset of the logic of `train_step()` since it omits the code that deals with updating the weights of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c923d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "st = time.time()\n",
    "def test_step(inputs, targets):\n",
    "    predictions = model(inputs, training=False)\n",
    "    loss = loss_fn(targets, predictions)\n",
    "\n",
    "    logs = {}\n",
    "    for metric in metrics:\n",
    "        metric.update_state(targets, predictions)\n",
    "        logs[\"val_\" + metric.name] = metric.result()\n",
    "\n",
    "    loss_tracking_metric.update_state(loss)\n",
    "    logs[\"val_loss\"] = loss_tracking_metric.result()\n",
    "    return logs\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_labels))\n",
    "val_dataset = val_dataset.batch(32)\n",
    "reset_metrics()\n",
    "for inputs_batch, targets_batch in val_dataset:\n",
    "    logs = test_step(inputs_batch, targets_batch)\n",
    "print(\"Evaluation results:\")\n",
    "for key, value in logs.items():\n",
    "    print(f\"...{key}: {value:.4f}\")\n",
    "print(\"Time:\",time.time()-st)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959bbb25",
   "metadata": {},
   "source": [
    "Our custom loops are running significantly slower than the built-in `fit()` and `evaluate()`, despite implementing essentially the same logic.  TensorFlow code is executed line by line, eagerly, much like regular Python code. It’s more performant to compile our TensorFlow code into a computation graph that can be globally optimized. The syntax to do this is very simple: we nned to add a `@tf.function` to any function we want to compile before executing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fd9a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "st = time.time()\n",
    "@tf.function\n",
    "def test_step(inputs, targets):\n",
    "    predictions = model(inputs, training=False)\n",
    "    loss = loss_fn(targets, predictions)\n",
    "\n",
    "    logs = {}\n",
    "    for metric in metrics:\n",
    "        metric.update_state(targets, predictions)\n",
    "        logs[\"val_\" + metric.name] = metric.result()\n",
    "\n",
    "    loss_tracking_metric.update_state(loss)\n",
    "    logs[\"val_loss\"] = loss_tracking_metric.result()\n",
    "    return logs\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_labels))\n",
    "val_dataset = val_dataset.batch(32)\n",
    "reset_metrics()\n",
    "for inputs_batch, targets_batch in val_dataset:\n",
    "    logs = test_step(inputs_batch, targets_batch)\n",
    "print(\"Evaluation results:\")\n",
    "for key, value in logs.items():\n",
    "    print(f\"...{key}: {value:.4f}\")\n",
    "print('Time:',time.time()-st)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62378e80",
   "metadata": {},
   "source": [
    "### Leveraging fit() with a custom training loop\n",
    "is a middle ground between `fit()` and a training loop written from scratch. We can provide a custom training step function and let the framework do the rest by overriding the `train_step()` method of the `Model` class. The `train_step()` is the function that is called by `fit()` for every batch of data. \n",
    "\n",
    "In the following example:\n",
    "- We create a new class that subclasses `keras.Model`.\n",
    "- We override the method `train_step(self, data)`. It returns a dictionary mapping metric names (including the loss) to their current values.\n",
    "- We implement a `metrics` property that tracks the model’s Metric instances. This enables the model to automatically call `reset_state()` on the model’s metrics at the start of each epoch and at the start of a call to `evaluate()`, so we don’t have to do it by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed91926c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
    "loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
    "\n",
    "class CustomModel(keras.Model):\n",
    "    def train_step(self, data):\n",
    "        inputs, targets = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self(inputs, training=True)\n",
    "            loss = loss_fn(targets, predictions)\n",
    "        gradients = tape.gradient(loss, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "\n",
    "        loss_tracker.update_state(loss)\n",
    "        return {\"loss\": loss_tracker.result()}\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [loss_tracker]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
