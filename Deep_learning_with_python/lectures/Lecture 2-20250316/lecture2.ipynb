{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49bda0d7-5796-4930-bb4f-833cba1935c5",
   "metadata": {},
   "source": [
    "# Lecture 2 - TensorFlow and Keras "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de2967c-3b05-4900-b0fa-663a7402c0b8",
   "metadata": {},
   "source": [
    "## TensorFlow\n",
    "*TensorFlow* is a Python-based, free, open source machine learning platform. It was released in November 2015. TensorFlow is much more than a single library. It’s really a platform, home to a vast ecosystem of components, some developed by Google and some developed by third parties.  Much like *NumPy*, the primary purpose of TensorFlow is to facilitate manipulating mathematical expressions over numerical tensors. But TensorFlow goes  beyond the scope of NumPy in the following ways:\n",
    "- It can automatically compute the gradient of any differentiable expressions.\n",
    "- It can run on CP on GPUs and TPUs.\n",
    "- Computation defined in TensorFlow can be easily distributed across many machines.\n",
    "- TensorFlow programs can be exported to other runtimes, such as C++, JavaScript, or TensorFlow Lite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7243b06b-827a-4bd0-b9c9-187c353ad1bd",
   "metadata": {},
   "source": [
    "### TensorFlow APIs\n",
    "TensorFlow enables us to do low-level tensor manipulation. TensorFlow APIs:\n",
    "- Tensors, including special tensors that store the network’s state (variables)\n",
    "- Tensor operations such as addition, relu, matmul\n",
    "- Backpropagation, a way to compute the gradient of mathematical expressions (handled in TensorFlow via the GradientTape object)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e743977-5d69-4b8c-8b13-6f23e91fef79",
   "metadata": {},
   "source": [
    "### Constant tensors and variables"
   ]
  },
  {
   "cell_type": "code",
   "id": "e329f8d7-2b67-472f-9902-a28cbc368b4d",
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "36e25e20-e141-43fe-bdeb-db0795f78381",
   "metadata": {},
   "source": [
    "#### All-ones or all-zeros tensors"
   ]
  },
  {
   "cell_type": "code",
   "id": "fe0ea499-a003-4df9-919e-22d6cf363645",
   "metadata": {},
   "source": [
    "x = tf.ones(shape=(2, 1))\n",
    "y = np.ones((2,1))\n",
    "print(\"x=\",x)\n",
    "print(\"y=\",y)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c23042b1-ff7f-48d5-b8ab-638f0d68964c",
   "metadata": {},
   "source": [
    "x = tf.zeros(shape=(2, 4, 3))\n",
    "y = np.zeros(shape=(2, 4, 3))\n",
    "x,y"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ec6bae8d-8b2e-4938-8d68-112cbeda8a8e",
   "metadata": {},
   "source": [
    "#### Random tensors"
   ]
  },
  {
   "cell_type": "code",
   "id": "a6802c2d-932c-4a94-a432-d28f54a56b53",
   "metadata": {},
   "source": [
    "x = tf.random.normal(shape=(3, 4), mean=0., stddev=1.)\n",
    "x"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2026a6a2-5388-4757-b88a-49eda251d269",
   "metadata": {},
   "source": [
    "x = tf.random.uniform(shape=(4, 6), minval=-10, maxval=10.)\n",
    "x"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ddcd9da2-fcc4-4592-be10-e8184d82d0c6",
   "metadata": {},
   "source": [
    "#### TensorFlow tensors aren’t assignable: they’re constant.  To manage modifiable state in TensorFlow we need class tf.Variable."
   ]
  },
  {
   "cell_type": "code",
   "id": "3bed56e7-ad95-4e5c-87bd-426b8c6e537e",
   "metadata": {},
   "source": [
    "x = np.ones(shape=(2, 2))\n",
    "x[0, 0] = 0"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cb663fee-e882-4d5b-877e-0175bd143210",
   "metadata": {},
   "source": [
    "x = tf.ones(shape=(2, 2))\n",
    "x[0, 0] = 0."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "22532092-bd44-4f44-8ed8-8dc552d86960",
   "metadata": {},
   "source": [
    "v = tf.Variable(initial_value=tf.random.normal(shape=(2, 3)))\n",
    "v"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1d5bd166-ca97-4766-8100-fb7d226697e0",
   "metadata": {},
   "source": [
    "v.assign(tf.ones((2, 3)))\n",
    "v"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fb3c32bf-cf23-4797-9d23-11f89c053c1e",
   "metadata": {},
   "source": [
    "v[0, 0].assign(0)\n",
    "v"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d2885ba2-cacb-412d-ada2-4eaf99898b62",
   "metadata": {},
   "source": [
    "v.assign_add(tf.ones((2, 3)))\n",
    "v"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fa7e679e-0d93-4801-8b06-e2f7eedccc7a",
   "metadata": {},
   "source": [
    "### Tensor operations"
   ]
  },
  {
   "cell_type": "code",
   "id": "40ab6d4a-48b7-429d-8e23-fd49363f79ed",
   "metadata": {},
   "source": [
    "a = tf.Variable(initial_value=[[1.,2.],[3.,4.]])\n",
    "a"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bf9c33ae-31a2-451e-a498-8706148ffc6f",
   "metadata": {},
   "source": [
    "b = tf.square(a)\n",
    "b"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "24e7a6b9-4db1-4eac-8e8c-d49c2b26f1db",
   "metadata": {},
   "source": [
    "c = tf.sqrt(a)\n",
    "c"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ff3f3655-1bb4-49aa-814c-a8e8efb9a1fe",
   "metadata": {},
   "source": [
    "d = b + c\n",
    "d"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a40af6f7-4241-4c55-8ea7-80bb0fc9e142",
   "metadata": {},
   "source": [
    "e = tf.matmul(b,c)\n",
    "e"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "af441bc8-5d63-498f-9761-1fb855d45468",
   "metadata": {},
   "source": [
    "e *= d\n",
    "e"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8be1d973",
   "metadata": {},
   "source": [
    "### The use of tensorflow for MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "id": "27d7e007",
   "metadata": {},
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "37d4eacf",
   "metadata": {},
   "source": [
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype(\"float32\") / 255  \n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype(\"float32\") / 255 "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "778d5c91",
   "metadata": {},
   "source": [
    "#### A DENSE CLASS"
   ]
  },
  {
   "cell_type": "code",
   "id": "5a3bc8a2",
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "  \n",
    "class NaiveDense:\n",
    "    '''creates two TensorFlow variables, W and b, and exposes a __call__() method that applies the preceding transformation'''\n",
    "    def __init__(self, input_size, output_size, activation):\n",
    "        self.activation = activation\n",
    "        #Create a matrix, W, of shape (input_size, output_size), initialized with random values\n",
    "        w_shape = (input_size, output_size)                                \n",
    "        w_initial_value = tf.random.uniform(w_shape, minval=0, maxval=1e-1)\n",
    "        self.W = tf.Variable(w_initial_value)\n",
    "        #Create a vector, b, of shape (output_size,), initialized with zeros\n",
    "        b_shape = (output_size,)                                          \n",
    "        b_initial_value = tf.zeros(b_shape)\n",
    "        self.b = tf.Variable(b_initial_value)\n",
    "  \n",
    "    #Apply the forward pass:\n",
    "    def __call__(self, inputs):                                          \n",
    "        return self.activation(tf.matmul(inputs, self.W) + self.b)\n",
    "  \n",
    "    #Convenience method for retrieving the layer’s weights:\n",
    "    @property\n",
    "    def weights(self):                                                     \n",
    "        return [self.W, self.b]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "14a76122",
   "metadata": {},
   "source": [
    "#### A SEQUENTIAL CLASS"
   ]
  },
  {
   "cell_type": "code",
   "id": "6d449fe8",
   "metadata": {},
   "source": [
    "class NaiveSequential:\n",
    "    def __init__(self, layers):\n",
    "        '''wraps a list of layers'''\n",
    "        self.layers = layers\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        '''calls the underlying layers on the inputs, in order'''\n",
    "        x = inputs\n",
    "        for layer in self.layers:\n",
    "           x = layer(x)\n",
    "        return x\n",
    "\n",
    "    @property \n",
    "    def weights(self):\n",
    "        '''keepa track of the layers’ parameters'''\n",
    "        weights = []\n",
    "        for layer in self.layers:\n",
    "           weights += layer.weights\n",
    "        return weights"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "eb1d3af3",
   "metadata": {},
   "source": [
    "Using this `NaiveDense` class and this `NaiveSequential` class, we can create a Keras model:"
   ]
  },
  {
   "cell_type": "code",
   "id": "b8e76f2e",
   "metadata": {},
   "source": [
    "model = NaiveSequential([\n",
    "    NaiveDense(input_size=28 * 28, output_size=512, activation=tf.nn.relu),\n",
    "    NaiveDense(input_size=512, output_size=10, activation=tf.nn.softmax)\n",
    "]) \n",
    "assert len(model.weights) == 4 "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "15d6b460",
   "metadata": {},
   "source": [
    "#### A BATCH GENERATOR"
   ]
  },
  {
   "cell_type": "code",
   "id": "109731d4",
   "metadata": {},
   "source": [
    "import math\n",
    "  \n",
    "class BatchGenerator:\n",
    "    def __init__(self, images, labels, batch_size=128):\n",
    "        assert len(images) == len(labels)\n",
    "        self.index = 0\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.num_batches = math.ceil(len(images) / batch_size)\n",
    " \n",
    "    def next(self):\n",
    "        images = self.images[self.index : self.index + self.batch_size]\n",
    "        labels = self.labels[self.index : self.index + self.batch_size]\n",
    "        self.index += self.batch_size\n",
    "        return images, labels"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "91176699",
   "metadata": {},
   "source": [
    "#### Running one training step"
   ]
  },
  {
   "cell_type": "code",
   "id": "783f2af7",
   "metadata": {},
   "source": [
    "def one_training_step(model, images_batch, labels_batch):\n",
    "    #Run the “forward pass” (compute the model’s predictions under a GradientTape scope):\n",
    "    with tf.GradientTape() as tape:                                         \n",
    "        predictions = model(images_batch)                                   \n",
    "        per_sample_losses = tf.keras.losses.sparse_categorical_crossentropy(labels_batch, predictions)                                      \n",
    "        average_loss = tf.reduce_mean(per_sample_losses)                    \n",
    "    gradients = tape.gradient(average_loss, model.weights)#Compute the gradient of the loss with regard to the weights                \n",
    "    update_weights(gradients, model.weights)#Update the weights using the gradients                                \n",
    "    return average_loss"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "30c7b05a",
   "metadata": {},
   "source": [
    "learning_rate = 1e-3 \n",
    "  \n",
    "def update_weights(gradients, weights):\n",
    "    for g, w in zip(gradients, weights):\n",
    "        w.assign_sub(g * learning_rate) #assign_sub is the equivalent of -= for TensorFlow variables"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "22144b30",
   "metadata": {},
   "source": [
    "We could use an Optimizer instance from Keras:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a1c92ead",
   "metadata": {},
   "source": [
    "from tensorflow.keras import optimizers\n",
    "  \n",
    "optimizer = optimizers.SGD(learning_rate=1e-3)\n",
    "  \n",
    "def update_weights(gradients, weights):\n",
    "    optimizer.apply_gradients(zip(gradients, weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b35d72",
   "metadata": {},
   "source": [
    "#### The full training loop:"
   ]
  },
  {
   "cell_type": "code",
   "id": "90c52e88",
   "metadata": {},
   "source": [
    "def fit(model, images, labels, epochs, batch_size=128):\n",
    "    for epoch_counter in range(epochs):\n",
    "        print(f\"Epoch {epoch_counter}\")\n",
    "        batch_generator = BatchGenerator(images, labels)\n",
    "        for batch_counter in range(batch_generator.num_batches):\n",
    "            images_batch, labels_batch = batch_generator.next()\n",
    "            loss = one_training_step(model, images_batch, labels_batch)\n",
    "            if batch_counter % 100 == 0:\n",
    "                print(f\"loss at batch {batch_counter}: {loss:.2f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dacbd8b1",
   "metadata": {},
   "source": [
    "fit(model, train_images, train_labels, epochs=10, batch_size=128)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f443d327",
   "metadata": {},
   "source": [
    "#### Evaluating the model:"
   ]
  },
  {
   "cell_type": "code",
   "id": "15cebedd",
   "metadata": {},
   "source": [
    "predictions = model(test_images)\n",
    "predictions = predictions.numpy()#converts it to a NumPy tensor\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "matches = predicted_labels == test_labels\n",
    "print(f\"accuracy: {matches.mean():.2f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6ceca841-68b0-4e00-939b-7a464d2d11b6",
   "metadata": {},
   "source": [
    "## Keras\n",
    "*Keras* was released in March 2015. It is a deep learning API for Python that provides a convenient way to define and train any kind of deep learning model. Keras was initially developed for research, with the aim of enabling fast deep learning experimentation. \n",
    "\n",
    "Keras was originally built on top of *Theano*, another tensor-manipulation library that provided automatic differentiation and GPU support—the earliest of its kind. Theano, developed at the Montréal Institute for Learning Algorithms (MILA) at the Université de Montréal, was a precursor of TensorFlow. It pioneered the idea of using static computation graphs for automatic differentiation and for compiling code to both CPU and GPU. In late 2015, after the release of TensorFlow, Keras was refactored to a multibackend architecture: it became possible to use Keras with either Theano or TensorFlow. By September 2016, TensorFlow had reached a level of technical maturity where it became possible to make it the default backend option for Keras.\n",
    "\n",
    "### Properties of Keras:\n",
    "- Through TensorFlow, Keras can run on top of different types of hardware - GPU, TPU, or CPU-and can be seamlessly scaled to thousands of machines.\n",
    "- Keras offers consistent and simple workflows, it minimizes the number of actions required for common use cases, and it provides clear and actionable feedback upon user error. This makes Keras easy to learn for a beginner, and highly productive to use for an expert.\n",
    "- Keras is used by academic researchers, engineers, and data scientists at Google, Netflix, Uber, CERN, NASA, Yelp, Instacart, Square, and hundreds of startups working on a wide range of problems across every industry.\n",
    "- Keras enables a wide range of different workflows, from the very high level to the very low level, corresponding to different user profiles.inux (WSL)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6865ec-70db-4dc7-98c7-27140ebc8d7a",
   "metadata": {},
   "source": [
    "### Keras APIs\n",
    "Keras APIs are used for high-level deep learning concepts:\n",
    "- Layers, which are combined into a model\n",
    "- A loss function, which defines the feedback signal used for learning\n",
    "- An optimizer, which determines how learning proceeds\n",
    "- Metrics to evaluate model performance, such as accuracy\n",
    "- A training loop that performs mini-batch stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4726eb-7492-4358-9159-12b2f042389b",
   "metadata": {},
   "source": [
    "## Practical issues concerning workspace\n",
    "It’s highly recommended to run deep learning code on a modern NVIDIA GPU rather than a computer’s CPU. Some applications—in particular, image processing with convolutional networks—will be excruciatingly slow on CPU, even a fast multicore CPU. There are three options to do deep learning on a GPU:\n",
    "\n",
    "Use the free GPU runtime from Colaboratory https://colab.research.google.com.\n",
    "Use GPU instances on Google Cloud or AWS EC2.\n",
    "Buy and install a physical NVIDIA GPU on your workstation.\n",
    "Colaboratory is the easiest way to get started, as it requires no hardware purchase and no software installation. However, the free version of Colaboratory is only suitable for small workloads. Running deep learning experiments in the cloud is a simple, low-cost way to move to larger workloads without having to buy any additional hardware. Nevertheless, this setup isn’t sustainable in the long term—or even for more than a few months. For heavy users of deep learning, setting up a local workstation with one or more GPUs is the best solution.\n",
    "\n",
    "Moreover, it’s better to be using a Unix workstation. Although it’s technically possible to run Keras on Windows directly, it is not recommended. To do deep learning on Windows workstation, the simplest solution is to set up an Ubuntu dual boot, or to leverage Windows Subsystem for Linux (WSL).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4df0a6-c312-4cbe-a4ad-d5d37f0a695a",
   "metadata": {},
   "source": [
    "## Layer\n",
    "is a fundamental data structure in neural networks. It is a data processing module that takes as input one or more tensors and that outputs one or more tensors. Some layers are stateless, but more frequently layers have a state: the layer’s weights, one or several tensors learned with stochastic gradient descent, which together contain the network’s knowledge.\n",
    "\n",
    "Different types of layers are appropriate for different tensor formats and different types of data processing:\n",
    "- simple vector data, stored in rank-2 tensors of shape (samples, features), is often processed by densely connected layers  (the Dense class in Keras);\n",
    "- sequence data, stored in rank-3 tensors of shape (samples, timesteps, features), is typically processed by recurrent layers, such as an LSTM layer, or 1D convolution layers (Conv1D)\n",
    "- image data, stored in rank-4 tensors, is usually processed by 2D convolution layers (Conv2D).\n",
    "\n",
    "A Layer is an object that encapsulates some state (weights) and some computation. The weights are typically defined in a build() (although they could also be created in the constructor, __init__()), and the computation is defined in the call() method."
   ]
  },
  {
   "cell_type": "code",
   "id": "a5fb88b5",
   "metadata": {},
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "class SimpleDense(keras.layers.Layer):#All Keras layers inherit from the base Layer class\n",
    "\n",
    "    def __init__(self, units, activation=None):\n",
    "        '''constructor'''\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "\n",
    "    def build(self, in_shape):\n",
    "        '''Weight creation'''\n",
    "        input_dim = in_shape[-1]\n",
    "        self.W = self.add_weight(shape=(input_dim, self.units), initializer=\"random_normal\")\n",
    "        self.b = self.add_weight(shape=(self.units,), initializer=\"zeros\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        '''the forward pass computation '''\n",
    "        y = tf.matmul(inputs, self.W) + self.b\n",
    "        if self.activation is not None:\n",
    "            y = self.activation(y)\n",
    "        return y"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6d12433d-cd41-463b-81d4-39d22a402579",
   "metadata": {},
   "source": [
    "Once instantiated, a layer like this can be used just like a function, taking as input a TensorFlow tensor:"
   ]
  },
  {
   "cell_type": "code",
   "id": "36ef7de2-ffd7-4c84-abc4-38f145f6f68d",
   "metadata": {},
   "source": [
    "my_dense = SimpleDense(units=32, activation=tf.nn.relu)\n",
    "input_tensor = tf.ones(shape=(2, 784))\n",
    "output_tensor = my_dense(input_tensor)\n",
    "print(my_dense.units)\n",
    "print(my_dense.W)\n",
    "print(my_dense.b)\n",
    "print(output_tensor)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5482de39-fad9-4b50-9016-d9d995ab93ca",
   "metadata": {},
   "source": [
    "When using Keras, we don’t have to worry about size compatibility most of the time, because the layers we add to models are dynamically built to match the shape of the incoming layer. Thus, the following model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a664e0cc",
   "metadata": {},
   "source": [
    "```python\n",
    "model = NaiveSequential([\n",
    "    NaiveDense(input_size=784, output_size=32, activation=\"relu\"),\n",
    "    NaiveDense(input_size=32, output_size=64, activation=\"relu\"),\n",
    "    NaiveDense(input_size=64, output_size=32, activation=\"relu\"),\n",
    "    NaiveDense(input_size=32, output_size=10, activation=\"softmax\")\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc67859",
   "metadata": {},
   "source": [
    "is equivalent to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058a06c2",
   "metadata": {},
   "source": [
    "```python\n",
    "model = keras.Sequential([\n",
    "    SimpleDense(32, activation=\"relu\"),\n",
    "    SimpleDense(64, activation=\"relu\"),\n",
    "    SimpleDense(32, activation=\"relu\"),\n",
    "    SimpleDense(2, activation=\"softmax\")\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cce5a74",
   "metadata": {},
   "source": [
    "## Model\n",
    "is a graph of layers and in Keras is represented by the Model class. The following are the most common network topologies:\n",
    "- Sequential models\n",
    "- Two-branch networks\n",
    "- Multihead networks\n",
    "- Residual connections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d769aa",
   "metadata": {},
   "source": [
    "The topology of a model defines a hypothesis space. By choosing a network topology, we constrain our space of possibilities (hypothesis space) to a specific series of tensor operations, mapping input data to output data. Thus, the architecture of our model is extremely important. However, picking the right network architecture is more an art than a science, and although there are some best practices and principles we can rely on, only practice can help become a proper neural-network architect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7bd01a",
   "metadata": {},
   "source": [
    "## Configuration of the learning process\n",
    "Once the model architecture is defined, we still have to choose three more things:\n",
    "- *Loss function* (objective function) — The quantity that will be minimized during training. It represents a measure of success for the task at hand.\n",
    "- *Optimizer* — Determines how the network will be updated based on the loss function. It implements a specific variant of stochastic gradient descent (SGD).\n",
    "- *Metrics* — The measures of success we want to monitor during training and validation, such as classification accuracy. Unlike the loss, training will not optimize directly for these metrics. As such, metrics don’t need to be differentiable.\n",
    "\n",
    "When loss, optimizer, and metrics are picked, we can use the built-in compile() and fit() methods to start training your model. The `compile()` method configures the training process and takes the arguments `optimizer`, `loss`, and `metrics`. For instance, we can configure the learning process as follows:"
   ]
  },
  {
   "cell_type": "code",
   "id": "b2ea566e",
   "metadata": {},
   "source": [
    "model = keras.Sequential([keras.layers.Dense(1)])\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"mean_squared_error\",\n",
    "              metrics=[\"accuracy\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "de4e5f80",
   "metadata": {},
   "source": [
    "or equivalently"
   ]
  },
  {
   "cell_type": "code",
   "id": "0102a491",
   "metadata": {},
   "source": [
    "model.compile(optimizer=keras.optimizers.RMSprop(),\n",
    "              loss=keras.losses.MeanSquaredError(),\n",
    "              metrics=[keras.metrics.BinaryAccuracy()])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "48fcbe86",
   "metadata": {},
   "source": [
    "We can also pass a learning_rate argument to the optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caa5fc0",
   "metadata": {},
   "source": [
    "```python\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-4),\n",
    "              loss=my_custom_loss,\n",
    "              metrics=[my_custom_metric_1, my_custom_metric_2])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4840a6c9",
   "metadata": {},
   "source": [
    "Generally, we won’t have to create our own losses, metrics, or optimizers from scratch, because Keras offers a wide range of built-in options:\n",
    "- Optimizers:\n",
    "    - SGD (with or without momentum)\n",
    "    - RMSprop\n",
    "    - Adam\n",
    "    - Adagrad\n",
    "- Losses:\n",
    "    - CategoricalCrossentropy\n",
    "    - SparseCategoricalCrossentropy\n",
    "    - BinaryCrossentropy\n",
    "    - MeanSquaredError\n",
    "    - KLDivergence\n",
    "    - CosineSimilarity\n",
    "- Metrics:\n",
    "    - CategoricalAccuracy\n",
    "    - SparseCategoricalAccuracy\n",
    "    - BinaryAccuracy\n",
    "    - AUC\n",
    "    - Precision\n",
    "    - Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a5bb59",
   "metadata": {},
   "source": [
    "Choosing the right loss function for the right problem is extremely important: our network will take any shortcut it can to minimize the loss, so if the objective doesn’t fully correlate with success for the task at hand, our network will end up doing things we may not have wanted. Fortunately, when it comes to common problems such as classification, regression, and sequence prediction, there are simple guidelines we can follow to choose the correct loss. For instance, we’ll use binary crossentropy for a two-class classification problem, categorical crossentropy for a many-class classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5937ee",
   "metadata": {},
   "source": [
    "After `compile()` comes `fit()`. The `fit()` method implements the training loop itself. These are its key arguments:\n",
    "- The data (inputs and targets) to train on. It will typically be passed either in the form of NumPy arrays or a TensorFlow Dataset object. \n",
    "- The number of epochs to train for: how many times the training loop should iterate over the data passed.\n",
    "- The batch size to use within each epoch of mini-batch gradient descent: the number of training examples considered to compute the gradients for one weight update step. "
   ]
  },
  {
   "cell_type": "code",
   "id": "7326dcc3",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "negative_samples = np.random.multivariate_normal(   \n",
    "    mean=[0, 3],                                    \n",
    "    cov=[[1, 0.5],[0.5, 1]],                        \n",
    "    size=1000) \n",
    "positive_samples = np.random.multivariate_normal(   \n",
    "    mean=[3, 0],                                    \n",
    "    cov=[[1, 0.5],[0.5, 1]],                        \n",
    "    size=1000) \n",
    "inputs = np.vstack((negative_samples, positive_samples)).astype(np.float32)\n",
    "targets = np.vstack((np.zeros((1000, 1), dtype=\"float32\"),\n",
    "                     np.ones((1000, 1), dtype=\"float32\")))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "79092073",
   "metadata": {},
   "source": [
    "history = model.fit(\n",
    "    inputs,          \n",
    "    targets,         \n",
    "    epochs=100,        \n",
    "    batch_size=128\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2b9aa80b",
   "metadata": {},
   "source": [
    "history.history"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7959dcd2",
   "metadata": {},
   "source": [
    "## Validation data\n",
    "The goal of machine learning is to obtain models that perform well in general, and particularly on data points that the model has never encountered before. Therefore it’s standard practice to reserve a subset of the training data as validation data. It is essential to keep the training data and validation data strictly separate."
   ]
  },
  {
   "cell_type": "code",
   "id": "6ede28e7",
   "metadata": {},
   "source": [
    "model = keras.Sequential([keras.layers.Dense(1)])\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=0.1),\n",
    "              loss=keras.losses.MeanSquaredError(),\n",
    "              metrics=[keras.metrics.BinaryAccuracy()])\n",
    "\n",
    "indices_permutation = np.random.permutation(len(inputs))\n",
    "shuffled_inputs = inputs[indices_permutation]\n",
    "shuffled_targets = targets[indices_permutation]\n",
    "\n",
    "num_validation_samples = int(0.3 * len(inputs))\n",
    "val_inputs = shuffled_inputs[:num_validation_samples]\n",
    "val_targets = shuffled_targets[:num_validation_samples]\n",
    "training_inputs = shuffled_inputs[num_validation_samples:]\n",
    "training_targets = shuffled_targets[num_validation_samples:]\n",
    "model.fit(\n",
    "    training_inputs,\n",
    "    training_targets,\n",
    "    epochs=5,\n",
    "    batch_size=16,\n",
    "    validation_data=(val_inputs, val_targets)\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0152e67f",
   "metadata": {},
   "source": [
    "To compute the validation loss and metrics after the training is complete, we can call the `evaluate()` method"
   ]
  },
  {
   "cell_type": "code",
   "id": "e35dfa3c",
   "metadata": {},
   "source": [
    "loss_and_metrics = model.evaluate(val_inputs, val_targets, batch_size=128)\n",
    "loss_and_metrics"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4ef7deb7",
   "metadata": {},
   "source": [
    "## Inference\n",
    "is the use of a model after training, in particular, to make predictions on new data. To do this, a naive approach would simply be to `__call__()` the model: `predictions = model(new_inputs)`. However, a better way to do inference is to use the `predict()` method since:\n",
    "- it will iterate over the data in small batches and return a `NumPy` array of predictions\n",
    "- unlike `__call__()`, it can also process TensorFlow Dataset objects."
   ]
  },
  {
   "cell_type": "code",
   "id": "2a56b10e",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "predictions = model.predict(val_inputs, batch_size=128)\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "ax1.scatter(val_inputs[:, 0], val_inputs[:, 1], c=val_targets)\n",
    "ax2.scatter(val_inputs[:, 0], val_inputs[:, 1], c=predictions[:, 0] > 0.5)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a8f9ffd7-5478-4480-9606-b27e7649b27d",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3d1a9c49-d829-4a35-ac30-bfdc1ed98027",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "431bcbc3-d494-4933-84df-148d9738aadb",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
