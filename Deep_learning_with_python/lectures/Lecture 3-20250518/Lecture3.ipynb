{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db0f06ce",
   "metadata": {},
   "source": [
    "# Lecture 3 - Generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e549f7",
   "metadata": {},
   "source": [
    "The fundamental issue in machine learning is the tension between *optimization* and *generalization*. *Optimization* refers to the process of adjusting a model to get the best performance possible on the training data, whereas *generalization* refers to how well the trained model performs on data it has never seen before. The purpose of a machine learning model is to get good generalization. If we fit the model to its training data too well, overfitting kicks in and generalization suffers.\n",
    "\n",
    "<img src=1.png/>\n",
    "\n",
    "At the beginning of training, optimization and generalization are correlated: the lower the loss on training data, the lower the loss on test data. While this is happening, a model is said to be underfit: there is still progress to be made. But after a certain number of iterations on the training data, generalization stops improving, validation metrics stall and then begin to degrade: the model is starting to overfit. Overfitting is particularly likely to occur when our data is noisy, if it involves uncertainty, or if it includes rare features.\n",
    "\n",
    "To illustrate this phenomenon let's consider the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e313d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T08:58:00.746692Z",
     "start_time": "2025-05-18T08:57:50.456379Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np\n",
    "\n",
    "(train_images, train_labels), _ = mnist.load_data()\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype(\"float32\") / 255\n",
    "\n",
    "train_images_with_noise_channels = np.concatenate(\n",
    "    [train_images, np.random.random((len(train_images), 28**2))], axis=1)\n",
    "\n",
    "train_images_with_zeros_channels = np.concatenate(\n",
    "    [train_images, np.zeros((len(train_images), 28**2))], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a95ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c2c169",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(512, activation=\"relu\"),\n",
    "        layers.Dense(10, activation=\"softmax\")\n",
    "    ])\n",
    "    model.compile(optimizer=\"rmsprop\",\n",
    "                  loss=\"sparse_categorical_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "model = get_model()\n",
    "history_noise = model.fit(\n",
    "    train_images_with_noise_channels, train_labels,\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    validation_split=0.2)\n",
    "\n",
    "model = get_model()\n",
    "history_zeros = model.fit(\n",
    "    train_images_with_zeros_channels, train_labels,\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f9cc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "val_acc_noise = history_noise.history[\"val_accuracy\"]\n",
    "val_acc_zeros = history_zeros.history[\"val_accuracy\"]\n",
    "epochs = range(1, 11)\n",
    "plt.plot(epochs, val_acc_noise, \"b-\",\n",
    "         label=\"Validation accuracy with noise channels\")\n",
    "plt.plot(epochs, val_acc_zeros, \"b--\",\n",
    "         label=\"Validation accuracy with zeros channels\")\n",
    "plt.title(\"Effect of noise channels on validation accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6167676",
   "metadata": {},
   "source": [
    "The following example shows that deep learning models can be trained to fit anything, as long as they have enough representational power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a02eadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), _ = mnist.load_data()\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype(\"float32\") / 255\n",
    "\n",
    "random_train_labels = train_labels[:]\n",
    "np.random.shuffle(random_train_labels)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(512, activation=\"relu\"),\n",
    "    layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit(train_images, random_train_labels,\n",
    "          epochs=100,\n",
    "          batch_size=128,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe63cdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, 101)\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "\n",
    "plt.plot(epochs, loss, \"b-\",\n",
    "         label=\"Loss for training data\")\n",
    "plt.plot(epochs, val_loss, \"b--\",\n",
    "         label=\"Loss for validation data\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece340e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = history.history[\"accuracy\"]\n",
    "val_accuracy = history.history[\"val_accuracy\"]\n",
    "plt.plot(epochs, accuracy, \"b-\",\n",
    "         label=\"Accuracy for training data\")\n",
    "plt.plot(epochs, val_accuracy, \"b--\",\n",
    "         label=\"Accuracy for validation data\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8cf66a",
   "metadata": {},
   "source": [
    "It turns out, the nature of generalization in deep learning has rather little to do with deep learning models themselves, and much to do with the structure of information in the real world. The *manifold hypothesis* posits that all natural data lies on a low-dimensional manifold within the high-dimensional space where it is encoded. The manifold hypothesis implies that\n",
    "- Machine learning models only have to fit relatively simple, low-dimensional, highly structured subspaces within their potential input space (latent manifolds).\n",
    "- Within one of these manifolds, it’s always possible to interpolate between two inputs, that is to say, morph one into another via a continuous path along which all points fall on the manifold.\n",
    "\n",
    "The ability to interpolate between samples is the key to understanding generalization in deep learning since we can start making sense of points we’ve never seen before by relating them to other points that lie close on the manifold. Humans are capable of extreme generalization, which is enabled by cognitive mechanisms other than interpolation: abstraction, symbolic models of the world, reasoning, logic, common sense, innate priors about the world, and pattern recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159f4349",
   "metadata": {},
   "source": [
    "## Training data suitable for generalization\n",
    "We’ll only be able to generalize if our data forms a manifold where points can be interpolated. The more informative and the less noisy features are, the more suitable to generalize. Model needs to be trained on a dense sampling of its input space. A “dense sampling” in this context means that the training data should densely cover the entirety of the input data manifold.\n",
    "\n",
    "<img src=2.png/>\n",
    "\n",
    "Summing up, we should always keep in mind that the best way to improve a deep learning model is to train it on more data or better data. When getting more data isn’t possible, the next best solution is to modulate the quantity of information that the model is allowed to store, or to add constraints on the smoothness of the model curve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e3d179",
   "metadata": {},
   "source": [
    "## Measuring the generalization - evaluating DL models\n",
    "Since our goal is to develop models that can successfully generalize to new data, it’s essential to be able to reliably measure the generalization power of models.\n",
    "Evaluating a model always boils down to splitting the available data into three sets: \n",
    "- training\n",
    "- validation \n",
    "- test\n",
    "\n",
    "We train on the training data and evaluate our model on the validation data. Once our model is ready for prime time, we test it one final time on the test data. We need to split data into these 3 sets since developing a model always involves tuning its configuration (choosing the number of layers or the size of the layers). We do this tuning by using as a feedback signal the performance of the model on the validation data. As a result, tuning the configuration of the model based on its performance on the validation set can quickly result in overfitting to the validation set. Therefore we care about performance on completely new data, not on the validation data, so we need to use the test dataset. \n",
    "There are 3 common ways to split data into training, validation, and test sets:\n",
    "- simple holdout validation, \n",
    "- K-fold validation, \n",
    "- iterated K-fold validation with shuffling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0648fbc9",
   "metadata": {},
   "source": [
    "### Simple holdout validation\n",
    "We set apart some fraction of our data as our test set, train on the remaining data, and evaluate on the test set.\n",
    "\n",
    "<img src=3.png/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28adffc3",
   "metadata": {},
   "source": [
    "```python\n",
    "# num_validation_samples is some fraction of len(data), usually 0.2-0.3\n",
    "num_validation_samples = 0.2\n",
    "np.random.shuffle(data)                                   \n",
    "validation_data = data[:num_validation_samples]           \n",
    "training_data = data[num_validation_samples:]             \n",
    "model = get_model()                                       \n",
    "model.fit(training_data, ...)                             \n",
    "validation_score = model.evaluate(validation_data, ...)    \n",
    "model = get_model()                                       \n",
    "model.fit(np.concatenate([training_data, validation_data]), ...)         \n",
    "test_score = model.evaluate(test_data, ...) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba43159",
   "metadata": {},
   "source": [
    "The simple holdout validation suffers from one flaw: if little data is available, then our validation and test sets may contain too few samples to be statistically representative of the data at hand. Therefore (iterated) $K$-fold validation was invented"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d5f444",
   "metadata": {},
   "source": [
    "### K-fold validation\n",
    "We split our data into $K$ partitions of equal size. For each partition $i$, train a model on the remaining $K - 1$ partitions, and evaluate it on partition $i$. Our final score is then the averages of the $K$ scores obtained.\n",
    "\n",
    "<img src=4.png/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff5cf4b",
   "metadata": {},
   "source": [
    "```python\n",
    "num_validation_samples = len(data) // k\n",
    "np.random.shuffle(data)\n",
    "validation_scores = [] \n",
    "for fold in range(k):\n",
    "    validation_data = data[num_validation_samples * fold: num_validation_samples * (fold + 1)] \n",
    "    training_data = np.concatenate( data[:num_validation_samples * fold], data[num_validation_samples * (fold + 1):])           \n",
    "    model = get_model()                                          \n",
    "    model.fit(training_data, ...)\n",
    "    validation_score = model.evaluate(validation_data, ...)\n",
    "    validation_scores.append(validation_score)\n",
    "validation_score = np.average(validation_scores)                 \n",
    "model = get_model()                                              \n",
    "model.fit(data, ...)                                             \n",
    "test_score = model.evaluate(test_data, ...) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6630bcb4",
   "metadata": {},
   "source": [
    "### Iterated K-fold validation with shuffling\n",
    "This method is suitable for situations in which we have relatively little data available and we need to evaluate our model as precisely as possible. It consists of applying $K$-fold validation multiple times, shuffling the data every time before splitting it $K$ ways. The final score is the average of the scores obtained at each run of $K$-fold validation. Thus we train and evaluate $P \\cdot K$ models (where $P$ is the number of iterations we use). So, it is the most time-expensive approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c366a9",
   "metadata": {},
   "source": [
    "### Best practices in model evaluation:\n",
    "- The use of common-sense baselines: before we start working with a dataset, we should always pick a trivial baseline that we’ll try to beat. This baseline could be the performance of a random classifier, or the performance of other non-machine learning technique. For instance, if we have a binary classification problem where 90% of samples belong to class A and 10% belong to class B, then a classifier that always predicts A already achieves 0.9 in validation accuracy, and we’ll need to do better than that.\n",
    "- Data representativeness:  we should randomly shuffle our data before splitting it into training and test sets unless we are trying to predict the future given the past.\n",
    "- The arrow of time: If we’re trying to predict the future given the past, we should not randomly shuffle our data before splitting it, because doing so will create a temporal leak: our model will effectively be trained on data from the future. In such situations, we should always make sure all data in our test set is posterior to the data in the training set.\n",
    "- Redundancy in data: If some data points in our data appear twice, then shuffling the data and splitting it into a training set and a validation set will result in redundancy between the training and validation sets. In effect, we’ll be testing on part of our training data. Therefore we make sure our training set and validation set are disjoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db5ed92",
   "metadata": {},
   "source": [
    "## Common problems during model fitting\n",
    "- Training loss doesn’t go down over time.\n",
    "- Training gets started just fine, but a model doesn’t meaningfully generalize: we can’t beat the common-sense baseline we set.\n",
    "- Training and validation loss both go down over time, and we can beat our baseline, but we don’t seem to be able to overfit, which indicates we’re still underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04fd280",
   "metadata": {},
   "source": [
    "### Tuning key gradient descent parameters\n",
    "When training doesn’t get started, or it stalls too early we can do the following:\n",
    "- Lowering or increasing the learning rate. A learning rate that is too high may lead to updates that vastly overshoot a proper fit, and a learning rate that is too low may make training so slow that it appears to stall.\n",
    "- Increasing the batch size. A batch with more samples will lead to gradients that are more informative and less noisy (lower variance).\n",
    "\n",
    "For instance, if we train the MNIST model with an inappropriately large learning rate of value 1, the model quickly reaches a training and validation accuracy in the 20%–30% range, but cannot get past that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4590cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), _ = mnist.load_data()\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype(\"float32\") / 255\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(512, activation=\"relu\"),\n",
    "    layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(1.),\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit(train_images, train_labels,\n",
    "          epochs=10,\n",
    "          batch_size=128,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374ca1f2",
   "metadata": {},
   "source": [
    "The same model with learning rate 0.01:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f676b9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Dense(512, activation=\"relu\"),\n",
    "    layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(1e-2),\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit(train_images, train_labels,\n",
    "          epochs=10,\n",
    "          batch_size=128,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecda41be",
   "metadata": {},
   "source": [
    "### Leveraging better architecture priors\n",
    "When our model  trains but doesn’t generalize it indicates that something is fundamentally wrong with our approach. This is perhaps the worst machine learning situation. Such situation may indicate the following problems:\n",
    "- The input data  doesn’t contain sufficient information to predict our targets: the problem as formulated is not solvable.\n",
    "- The model we’re using is not suited for the problem. For instance, in a timeseries prediction problem, a densely connected architecture isn’t able to beat a trivial baseline, whereas a more appropriate recurrent architecture does manage to generalize well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd88e686",
   "metadata": {},
   "source": [
    "### Increasing model capacity\n",
    "If training and validation loss both go down over time, increasing model capacity may be a necessity for solving this issue.\n",
    "We can see this phenomenon in the following logistic regression model on MNIST, where validation metrics seem to stall, or to improve very slowly, instead of peaking and reversing course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffc8d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([layers.Dense(10, activation=\"softmax\")])\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "n_epochs = 50\n",
    "history_small_model = model.fit(\n",
    "    train_images, train_labels,\n",
    "    epochs=n_epochs,\n",
    "    batch_size=128,\n",
    "    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63067e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss = history_small_model.history[\"val_loss\"]\n",
    "loss = history_small_model.history[\"loss\"]\n",
    "epochs = range(1, n_epochs+1)\n",
    "plt.plot(epochs, loss, \"b-\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, \"r--\", label=\"Validation loss\")\n",
    "plt.title(\"Effect of insufficient model capacity on validation loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a340e3e",
   "metadata": {},
   "source": [
    "If we can’t overfit, it’s likely a problem with the representational power of our model. We’re going to need a bigger model, one with more capacity (able to store more information). We can increase representational power by:\n",
    "- adding more layers, \n",
    "- using bigger layers (layers with more parameters), \n",
    "- using kinds of layers that are more appropriate for the problem at hand (better architecture priors).\n",
    "\n",
    "If we add two intermediate layers with 96 units each in the previous model, the problem vanishes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf09f6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Dense(96, activation=\"relu\"),\n",
    "    layers.Dense(96, activation=\"relu\"),\n",
    "    layers.Dense(10, activation=\"softmax\"),\n",
    "])\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "history_large_model = model.fit(\n",
    "    train_images, train_labels,\n",
    "    epochs=n_epochs,\n",
    "    batch_size=128,\n",
    "    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ab81bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss = history_large_model.history[\"val_loss\"]\n",
    "loss = history_large_model.history[\"loss\"]\n",
    "epochs = range(1, n_epochs+1)\n",
    "plt.plot(epochs, loss, \"b-\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, \"r--\", label=\"Validation loss\")\n",
    "plt.title(\"Effect of insufficient model capacity on validation loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776b882c",
   "metadata": {},
   "source": [
    "## Improving generalization\n",
    "Once our model has shown itself to have some generalization power and to be able to overfit, we can deal with maximizing generalization. To improve generalization power of model we can use the following techniques:\n",
    "- dataset curation\n",
    "- feature engineering\n",
    "- using early stopping\n",
    "- regularizing model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fc1dc3",
   "metadata": {},
   "source": [
    "### Dataset curation:\n",
    "- Data should make it possible to smoothly interpolate between samples. If a problem is overly noisy or discrete, deep learning will be useless.\n",
    "- Make sure we have enough data: we need a dense sampling of the input-cross-output space.\n",
    "- Minimize labeling errors.\n",
    "- Clean our data and deal with missing values.\n",
    "- If we have many features and we aren’t sure which ones are actually useful, we do feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bd6f11",
   "metadata": {},
   "source": [
    "### Feature engineering\n",
    "is the process of using our own knowledge about the data and about the machine learning algorithm at hand to make the algorithm work better by applying hardcoded (non-learned) transformations to the data before it goes into the model. The data needs to be presented to the model in a way that will make the model’s job easier.\n",
    "\n",
    "<img src=5.png/>\n",
    "\n",
    "If we choose to use the raw pixels of the image as input data, we have a difficult machine learning problem, demanding a convolutional neural network to solve it. If we use polar coordinates with regard to the center of the image our input will become the angle theta of each clock hand. At this point, our features are making the problem so easy that no machine learning is required.\n",
    "\n",
    "Before deep learning, feature engineering was the most important part of the machine learning workflow, because classical shallow algorithms didn’t have hypothesis spaces rich enough to learn useful features by themselves. The way we presented the data to the algorithm was absolutely critical to its success.\n",
    "\n",
    "Although modern deep learning removes the need for most feature engineering, because neural networks are capable of automatically extracting useful features from raw data, we still need to consider feature engineering due to:\n",
    "- Good features allow us to solve problems more elegantly while using fewer resources. \n",
    "- Good features let us solve a problem with far less data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3de9700",
   "metadata": {},
   "source": [
    "### Using early stopping\n",
    "We never fully fit a deep learning model. Such a fit wouldn’t generalize at all. We will always interrupt training long before we’ve reached the minimum possible training loss. Finding the exact point during training where we’ve reached the most generalizable fit is one of the most effective ways to improve generalization. This aim can be executed by:\n",
    "- training our models for longer than needed to figure out the number of epochs that yielded the best validation metrics, and then we would retrain a new model for exactly that number of epochs (however, it requires us to do redundant work, which can sometimes be expensive);\n",
    "- saving model at the end of each epoch, and once we’ve found the best epoch, reuse the closest saved model;\n",
    "- using EarlyStopping callback in Keras that will interrupt training as soon as validation metrics have stopped improving, while remembering the best known model state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5319d5e",
   "metadata": {},
   "source": [
    "### Regularizing model\n",
    "Regularization techniques are a set of best practices that actively impede the model’s ability to fit perfectly to the training data, with the goal of making the model perform better during validation. The most common regularization techniques are as follows:\n",
    "- REDUCING THE NETWORK’S SIZE\n",
    "- ADDING WEIGHT REGULARIZATION\n",
    "- ADDING DROPOUT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba4a7c5",
   "metadata": {},
   "source": [
    "#### REDUCING THE NETWORK’S SIZE\n",
    "The simplest way to mitigate overfitting is to reduce the size of the model (the number of learnable parameters in the model, determined by the number of layers and the number of units per layer).  At the same time, keep in mind that we should use models that have enough parameters that they don’t underfit. There is no magical formula to determine the right number of layers or the right size for each layer.  The general workflow for finding an appropriate model size is to start with relatively few layers and parameters, and increase the size of the layers or add new layers until we see diminishing returns with regard to validation loss. We’ll know our model is too large if it starts overfitting right away and if its validation loss curve looks choppy with high-variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbc62e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "(train_data, train_labels), _ = imdb.load_data(num_words=10000)\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "train_data = vectorize_sequences(train_data)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "history_original = model.fit(train_data, train_labels,\n",
    "                             epochs=20, batch_size=512, validation_split=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d087ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Version of the model with lower capacity\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(4, activation=\"relu\"),\n",
    "    layers.Dense(4, activation=\"relu\"),\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "history_smaller_model = model.fit(\n",
    "    train_data, train_labels,\n",
    "    epochs=20, batch_size=512, validation_split=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721dbe4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, 21)\n",
    "plt.plot(epochs, history_original.history[\"val_loss\"], \"b-\",\n",
    "         label=\"Validation loss for the original model\")\n",
    "plt.plot(epochs, history_smaller_model.history[\"val_loss\"], \"b--\",\n",
    "         label=\"Validation loss for the smaller model\")\n",
    "plt.title(\"Effect of size on validation loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6015f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version of the model with higher capacity\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(512, activation=\"relu\"),\n",
    "    layers.Dense(512, activation=\"relu\"),\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "history_larger_model = model.fit(\n",
    "    train_data, train_labels,\n",
    "    epochs=20, batch_size=512, validation_split=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f230fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, 21)\n",
    "plt.plot(epochs, history_original.history[\"val_loss\"], \"b-\",\n",
    "         label=\"Validation loss of the original model\")\n",
    "plt.plot(epochs, history_larger_model.history[\"val_loss\"], \"b--\",\n",
    "         label=\"Validation loss of the larger model\")\n",
    "plt.title(\"Effect of size on validation loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed018aec",
   "metadata": {},
   "source": [
    "#### ADDING WEIGHT REGULARIZATION\n",
    "is a way to mitigate overfitting by putting constraints on the complexity of a model by forcing its weights to take only small values, which makes the distribution of weight values more regular. It’s done by adding to the loss function of the model a cost associated with having large weights:\n",
    "- L1 regularization—The cost added is proportional to the absolute value of the weight coefficients.\n",
    "- L2 regularization—The cost added is proportional to the square of the value of the weight coefficients.\n",
    "\n",
    "In Keras, weight regularization is added by passing weight regularizer instances to layers as keyword arguments. For instance, we can add L2 weight regularization to the previous model as follows. In this code, `l2(0.002)` means every coefficient in the weight matrix of the layer will add `0.002 * weight_coefficient_value ** 2` to the total loss of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b56978",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(16,\n",
    "                 kernel_regularizer=regularizers.l2(0.002),\n",
    "                 activation=\"relu\"),\n",
    "    layers.Dense(16,\n",
    "                 kernel_regularizer=regularizers.l2(0.002),\n",
    "                 activation=\"relu\"),\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "history_l2_reg = model.fit(\n",
    "    train_data, train_labels,\n",
    "    epochs=20, batch_size=512, validation_split=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cdfb12",
   "metadata": {},
   "source": [
    "We can see that the model with L2 regularization has become much more resistant to overfitting than the reference model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3979ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, 21)\n",
    "plt.plot(epochs, history_original.history[\"val_loss\"], \"b-\",\n",
    "         label=\"Validation loss of the original model\")\n",
    "plt.plot(epochs, history_l2_reg.history[\"val_loss\"], \"b--\",\n",
    "         label=\"Validation loss of L2-regularized model\")\n",
    "plt.title(\"Effect of size on validation loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dba36d",
   "metadata": {},
   "source": [
    "Apart from L1 and L2 regularizers we can also add simultaneous L1 and L2 regularization:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9985831",
   "metadata": {},
   "source": [
    "```python\n",
    "from tensorflow.keras import regularizers\n",
    "regularizers.l1_l2(l1=0.001, l2=0.001) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734d0ce9",
   "metadata": {},
   "source": [
    "#### ADDING DROPOUT\n",
    "Weight regularization is more typically used for smaller deep learning models. Large deep learning models tend to be so overparameterized that imposing constraints on weight values hasn’t much impact on model capacity and generalization. In such cases dropout is preferred. Dropout is one of the most effective and most commonly used regularization techniques for neural networks. Dropout, applied to a layer, consists of randomly dropping out (setting to zero) a number of output features of the layer during training. Let’s say a given layer would normally return a vector `[0.2, 0.5, 1.3, 0.8, 1.1]` for a given input sample during training. After applying dropout, this vector will have a few zero entries distributed at random: for example, `[0, 0.5, 1.3, 0, 1.1]`. The dropout rate is the fraction of the features that are zeroed out; it’s usually set between 0.2 and 0.5. At test time, no units are dropped out; instead, the layer’s output values are scaled down by a factor equal to the dropout rate, to balance for the fact that more units are active than at training time.\n",
    "\n",
    "In Keras, we can introduce dropout in a model via the Dropout layer, which is applied to the output of the layer right before it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bd4271",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "history_dropout = model.fit(\n",
    "    train_data, train_labels,\n",
    "    epochs=20, batch_size=512, validation_split=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b48dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, 21)\n",
    "plt.plot(epochs, history_original.history[\"val_loss\"], \"b-\",\n",
    "         label=\"Validation loss of the original model\")\n",
    "plt.plot(epochs, history_dropout.history[\"val_loss\"], \"b--\",\n",
    "         label=\"Validation loss of dropout-regularized model\")\n",
    "plt.title(\"Effect of size on validation loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
