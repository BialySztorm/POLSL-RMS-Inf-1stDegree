{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "091c77b9-a576-440c-abb4-d3acd00f03bb",
   "metadata": {},
   "source": [
    "# Lecture 1A - The fundamentals of deep learning in the broader context of AI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f54de9-d90a-416c-a1ee-6c205583448c",
   "metadata": {},
   "source": [
    "## Artificial intelligence\n",
    "**Artificial intelligence (AI)** can be described as the effort to automate intellectual tasks normally performed by humans. As such, AI is a general field that encompasses machine learning and deep learning, but that also includes many more approaches that may not involve any learning. \n",
    "One of such approaches is so called **symbolic AI** that was the dominant paradigm in AI from the 1950s to the late 1980s, and it reached its peak popularity during the expert systems boom of the 1980s. Most experts dealing with symbolic AI believed that human-level artificial intelligence could be achieved by  a sufficiently large set of explicit rules stored in explicit databases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326ca4c7-e098-4827-afb3-5bd81f649cd9",
   "metadata": {},
   "source": [
    "<img src=\"1.png\" alt=\"The relation between artificial intelligence, machine learning, and deep learning\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96acb4a4-85c8-41cb-92c6-02f3abe9835e",
   "metadata": {},
   "source": [
    "## Machine learning \n",
    "### ML and symbolic AI\n",
    "The usual way to make a computer do useful work is to have a human programmer write down a computer program to be followed to turn input data into appropriate answers. Machine learning turns this around: the machine looks at the input data and the corresponding answers, and figures out what the rules should be.  A machine learning system is trained rather than explicitly programmed. It’s presented with many examples relevant to a task, and it finds statistical structure in these examples that eventually allows the system to come up with rules for automating the task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0577cc14-7f54-4956-bd15-77eab232d157",
   "metadata": {},
   "source": [
    "<img src=\"2.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1561088-b7d2-4e47-b56b-80d7757e349e",
   "metadata": {},
   "source": [
    "### ML and statistics\n",
    "Machine learning has started to flourish in the 1990s and has quickly become the most popular and successful subfield of AI, thanks to faster hardware and larger datasets. Machine learning is related to mathematical statistics. However, unlike statistics, machine learning tends to deal with large, complex datasets for which classical statistical analysis would be impractical. Moreover, machine learning, especially deep learning, exhibits comparatively little mathematical theory and is fundamentally an engineering discipline driven by empirical findings and deeply reliant on advances in software and hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d621bf4-29ff-41d5-bcb8-1b0e876a679d",
   "metadata": {},
   "source": [
    "### Each machine learning algorithm needs 3 elements:\n",
    "- Input data points\n",
    "- Examples of the expected output\n",
    "- A way to measure whether the algorithm is doing a good job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ea641b-4d3a-44d4-b0ac-720e3c4b1fcd",
   "metadata": {},
   "source": [
    "### The central problem in machine learning\n",
    "A machine learning model transforms its input data into meaningful outputs, a process that is “learned” from exposure to known examples of inputs and outputs. Therefore, the central problem in machine learning is to meaningfully transform data: in other words, to learn useful representations of the input data at hand—representations that get us closer to the expected output. Machine learning algorithms usually do not find these transformations; they’re merely searching through a predefined set of operations, called a hypothesis space. So ML can be defined as searching for useful representations and rules over some input data, within a predefined space of possibilities, using guidance from a feedback signal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572df300-523a-4dde-8577-1bee3fde0b9d",
   "metadata": {},
   "source": [
    "<img src=\"3.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ac4c4c-cdd0-4ee2-a64b-a05ed6796cfb",
   "metadata": {},
   "source": [
    "## Other ML methods\n",
    "Most of the machine learning algorithms used in the industry today aren’t deep learning algorithms. Deep learning isn’t always the right tool for the job. \n",
    "### Probabilistic modeling \n",
    "is the application of the principles of statistics to data analysis. It is one of the earliest forms of machine learning, and it’s still widely used to this day. One of the best-known algorithms in this category are the Naive Bayes algorithm and logistic regression.\n",
    "### Kernel methods\n",
    "are a group of classification algorithms, the best known of which is the Support Vector Machine (SVM). The modern formulation of an SVM was developed by Vladimir Vapnik and Corinna Cortes in the early 1990s at Bell Labs and published in 1995. SVM finds decision boundaries separating two classes in two steps:\n",
    "1. The data is mapped to a new high-dimensional representation where the decision boundary can be expressed as a hyperplane.\n",
    "2. A good decision boundary (a separation hyperplane) is computed by trying to maximize the distance between the hyperplane and the closest data points from each class, a step called maximizing the margin.\n",
    "<img src=\"4.png\"/>\n",
    "\n",
    "The technique of mapping data to a high-dimensional representation where a classification problem becomes simpler is often computationally intractable. Therefore the kernel functions emerged. They are computationally tractable operations that maps any two points in initial space to the distance between two points in target representation space, completely bypassing the explicit computation of the new representation\n",
    "### Decision trees\n",
    "are flowchart-like structures that let you classify input data points or predict output values given inputs.\n",
    "They’re easy to visualize and interpret.\n",
    "\n",
    "<img src=\"5.png\"/>\n",
    "\n",
    "### Random forests\n",
    "introduced a robust, practical take on decision-tree learning that involves building a large number of specialized decision trees and then ensembling their outputs. They’re almost always the second-best algorithm for any shallow machine learning task.\n",
    "### Gradient boosting machine\n",
    "is a machine learning technique based on ensembling weak prediction models, generally decision trees. It uses gradient boosting, a way to improve any machine learning model by iteratively training new models that specialize in addressing the weak points of the previous models. It is one of the best, if not the best, algorithm for dealing with nonperceptual data today. Alongside deep learning, it’s one of the most commonly used techniques in <a href=\"http://kaggle.com\">Kaggle</a> competitions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86b3208-b55f-484a-91bc-8a63183e70e0",
   "metadata": {},
   "source": [
    "## Deep learning\n",
    "Deep learning is a subfied of machine learning methods and is based on the idea of successive *layers* of representations. The number of layers that contribute to a model of the data is called the *depth* of the model. Modern deep learning often involves even hundreds of successive layers of representations, while, other machine learning methods tend to focus on learning only one or two layers of representations of the data.  These layered representations are learned via models called *neural networks*. Summarizing, deep neural networks  map inputs to targets (which is done by observing many examples of input and targets) via a deep sequence of simple data transformations (layers).\n",
    "\n",
    "<img src=\"6.png\"/>\n",
    "\n",
    "### Weigths\n",
    "The specification of what a layer does to its input data is stored in the layer’s weights. The transformation implemented by a layer is parameterized by its *weights*, that are also called the parameters of a layer. In this context, *learning* means finding a set of values for the weights of all layers in a network, such that the network correctly maps example inputs to their associated targets. \n",
    "\n",
    "### Loss function\n",
    "The *loss function* of the network, also called the *objective function* or *cost function* is designed to control the output of a neural network, you need to be able to measure how far this output is from what you expected. The loss function takes the predictions of the network and the true target  and computes a distance score, capturing how well the network has done on this specific example.\n",
    "\n",
    "### Optimizer\n",
    "The *optimizer*, implements the *Backpropagation algorithm*. It uses a score as a feedback signal to adjust the value of the weights a little, in a direction that will lower the loss score for the current example. Initially, the weights of the network are assigned random values. With every example the network processes, the weights are adjusted a little in the correct direction, and the loss score decreases. This is the training loop, which, repeated a sufficient number of times (typically tens of iterations over thousands of examples), yields weight values that minimize the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f00fb6-539a-4874-a3c7-500c12db3725",
   "metadata": {},
   "source": [
    "# Lecture 1B - mathematical aspects of deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9514e85b-9f55-4096-bb0a-9206498caf42",
   "metadata": {},
   "source": [
    "## Tensors\n",
    "All current machine learning systems use *tensors* as their basic data structure. A tensor is a container for data—usually numerical data. Tensors are a generalization of matrices (rank-2 tensors) to an arbitrary number of dimensions. In Python, tensors can be represented by NumPy arrays. In particular, scalars are rank-0 tensors and vectors are rank-1 tensors. \n",
    "\n",
    "A tensor is defined by three key attributes:\n",
    "- Number of axes (rank). This is also called the tensor’s ndim in Python libraries such as NumPy or TensorFlow.\n",
    "- Shape—This is a tuple of integers that describes how many dimensions the tensor has along each axis.\n",
    "- Data type (usually called dtype in Python libraries)—This is the type of the data contained in the tensor; for instance, a tensor’s type could be float16, float32, float64, uint8, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497377b1-1c3a-4f4b-b7d5-00ebe31df741",
   "metadata": {},
   "source": [
    "### Example of a tensor: array of 60,000 matrices of 28 × 28 integers:"
   ]
  },
  {
   "cell_type": "code",
   "id": "a4896bb3-5188-484f-93d5-8468b4bd60a2",
   "metadata": {},
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "539a6564-8233-43a2-b019-91665809cd8f",
   "metadata": {},
   "source": [
    "train_images.ndim #rank"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "08fa9852-e7bd-420a-8766-73ab2706833a",
   "metadata": {},
   "source": [
    "train_images.shape #shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "84345bbf-cac0-4519-9d8d-8db278f6b974",
   "metadata": {},
   "source": [
    "train_images.dtype #data type"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7623dbb6-b4ee-4a26-8ff6-d6ac1b76e113",
   "metadata": {},
   "source": [
    "Each such matrix is a grayscale image, with coefficients between 0 and 255:"
   ]
  },
  {
   "cell_type": "code",
   "id": "0641f8c2-bb87-48a6-b47e-97c1fb4cc794",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for i in range(10):\n",
    "    digit = train_images[i]\n",
    "    plt.imshow(digit, cmap=plt.cm.binary)\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5ccf9908-0cbe-4434-ace6-f08f37a782ad",
   "metadata": {},
   "source": [
    "### Slicing\n",
    "Selecting specific elements in a tensor is called tensor *slicing*. "
   ]
  },
  {
   "cell_type": "code",
   "id": "34d0e6e9-3bf0-4fef-9d72-339ed3ffd52a",
   "metadata": {},
   "source": [
    "slice = train_images[15:200, :, :]\n",
    "slice.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0e8dc4c1-b96b-4d32-b9d1-f2a46a7f1445",
   "metadata": {},
   "source": [
    "### Batches\n",
    "Deep learning models don’t process an entire dataset at once; rather, they break the data into *batches*. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3a1698-3f79-4e41-9156-72d7f5360c44",
   "metadata": {},
   "source": [
    "### Examples of data tensors\n",
    "The data we manipulate almost always fall into one of the following categories:f images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a8e085-ae56-47c3-96eb-1f4e27ce0aa8",
   "metadata": {},
   "source": [
    "#### Vector \n",
    "data—Rank-2 tensors of shape (samples, features), where each sample is a vector of numerical attributes (“features”)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8675f8-b1cf-4f1e-a2f5-1197ef036f11",
   "metadata": {},
   "source": [
    "#### Timeseries data \n",
    "sequence data—Rank-3 tensors of shape (samples, timesteps, features), where each sample is a sequence (of length timesteps) of feature vectors\n",
    "\n",
    "<br/><img src=\"7.png\"/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f733aa46-3f61-41d8-ad6d-a53152bffcd1",
   "metadata": {},
   "source": [
    "#### Images \n",
    "Rank-4 tensors of shape (samples, height, width, channels), where each sample is a 2D grid of pixels, and each pixel is represented by a vector of values (“channels”)\n",
    "\n",
    "<br/><img src=\"8.png\"/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a78138-f3a4-4708-b7da-546b879c6338",
   "metadata": {},
   "source": [
    "#### Video \n",
    "Rank-5 tensors of shape (samples, frames, height, width, channels), where each sample is a sequence (of length frames) of images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f70879a-8489-4fd7-824d-acfb554c2933",
   "metadata": {},
   "source": [
    "### Tensor operations\n",
    "All transformations learned by deep neural networks can be reduced to a handful of tensor operations (or tensor functions) applied to tensors of numeric data. \n",
    "\n",
    "The most common tensor operations:\n",
    "- a dot product of tensors\n",
    "- an addition  between a tensor and another tensor.\n",
    "- tensor reshaping\n",
    "\n",
    "All operations on tesnors are element-wise operations. It means that they are applied independently to each entry in the tensors being considered, i.e., they are highly amenable to massively parallel implementations (vectorized implementations). These operations are available as well-optimized built-in NumPy functions, which themselves delegate the heavy lifting to a Basic Linear Algebra Subprograms (BLAS) implementation. BLAS are low-level, highly parallel, efficient tensor-manipulation routines that are typically implemented in Fortran or C."
   ]
  },
  {
   "cell_type": "code",
   "id": "9cb9d970-fe0a-4d5e-a883-37114461f3d2",
   "metadata": {},
   "source": [
    "def naive_add(x, y):\n",
    "    assert len(x.shape) == 2\n",
    "    assert x.shape == y.shape\n",
    "    x = x.copy()\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i, j] += y[i, j]\n",
    "    return x\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "x = np.random.random((50, 100))\n",
    "y = np.random.random((50, 100))\n",
    "  \n",
    "t0 = time.time() \n",
    "for _ in range(1000):\n",
    "    z = x + y\n",
    "t1 = time.time() - t0\n",
    "\n",
    "t0 = time.time() \n",
    "for _ in range(1000):\n",
    "    z = naive_add(x, y)\n",
    "t2 = time.time() - t0\n",
    "\n",
    "t1,t2,t2/t1"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "daba93c5-b249-4b91-8659-a5d3b7c7eda8",
   "metadata": {},
   "source": [
    "def naive_vector_dot(x, y):\n",
    "    assert len(x.shape) == 1         \n",
    "    assert len(y.shape) == 1         \n",
    "    assert x.shape[0] == y.shape[0]\n",
    "    z = 0. \n",
    "    for i in range(x.shape[0]):\n",
    "        z += x[i] * y[i]\n",
    "    return z\n",
    "def naive_matrix_dot(x, y):\n",
    "    assert len(x.shape) == 2                  \n",
    "    assert len(y.shape) == 2                  \n",
    "    assert x.shape[1] == y.shape[0]           \n",
    "    z = np.zeros((x.shape[0], y.shape[1]))    \n",
    "    for i in range(x.shape[0]):               \n",
    "        for j in range(y.shape[1]):           \n",
    "            row_x = x[i, :]\n",
    "            column_y = y[:, j]\n",
    "            z[i, j] = naive_vector_dot(row_x, column_y)\n",
    "    return z\n",
    "\n",
    "X = np.random.random((4, 6))\n",
    "Y = np.random.random((6, 7))\n",
    "\n",
    "X,Y,X.dot(Y),naive_matrix_dot(X, Y)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3df9d930-3c39-4530-8090-9a13db482988",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "X = np.random.random((40, 60))\n",
    "Y = np.random.random((60, 70))\n",
    "t0 = time.time() \n",
    "for _ in range(10**3):\n",
    "    X.dot(Y)\n",
    "t1 = time.time() - t0\n",
    "\n",
    "t0 = time.time() \n",
    "for _ in range(10**3):\n",
    "    naive_matrix_dot(X, Y)\n",
    "t2 = time.time() - t0\n",
    "\n",
    "t1,t2,t2/t1"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2ea3896b-dca2-48b6-b3ce-5d30db49c4ca",
   "metadata": {},
   "source": [
    "x = np.array(range(12))\n",
    "x.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0a2ad16a-9dfc-41bc-8caa-ddd8d118408e",
   "metadata": {},
   "source": [
    "x.reshape(3,4)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0d34f878-e491-4769-972e-51a799b30186",
   "metadata": {},
   "source": [
    "x.reshape(6,2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c73cb727-f8b6-4fc2-bff5-d6a969baf41d",
   "metadata": {},
   "source": [
    "Another mechanism used in tensor operations is *broadcasting*. When possible, and if there’s no ambiguity, the smaller tensor will be broadcast to match the shape of the larger tensor. Broadcasting consists of two steps:\n",
    "\n",
    "- Axes (called broadcast axes) are added to the smaller tensor to match the ndim of the larger tensor.\n",
    "- The smaller tensor is repeated alongside these new axes to match the full shape of the larger tensor."
   ]
  },
  {
   "cell_type": "code",
   "id": "f2029646-971f-474d-8a8f-29c004ccd0b3",
   "metadata": {},
   "source": [
    "X = np.random.choice(range(11),(2,5))\n",
    "y = np.random.choice(range(11),(5,))\n",
    "X,y,X+y"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e3626ff5-23ea-449f-9997-24d5c62a3792",
   "metadata": {},
   "source": [
    "## Affine transform\n",
    "An affine transform is the combination of a linear transform (achieved via a dot product with some matrix) and a translation (achieved via a vector addition):  $$y = W \\cdot  x + b$$\n",
    "A Dense layer without an activation function is an affine layer.\n",
    "<br/><img src=\"9.png\"/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9129224e-1552-4585-8a23-6aba4cf4be41",
   "metadata": {},
   "source": [
    "## Activation functions\n",
    "If you apply many of them repeatedly, you still end up with an affine transform (so you could just have applied that one affine transform in the first place):\n",
    "$$y = W_1 \\cdot  (W_2 \\cdot  x + b_2) + b_1 = W_1 \\cdot W_2 \\cdot x + W_1 \\cdot b_2 + b_1$$\n",
    "As a consequence, a multilayer neural network made entirely of Dense layers without activations would be equivalent to a single Dense layer. This “deep” neural network would just be a linear model in disguise! This is why we need activation functions.\n",
    "\n",
    "Three major types of activation functions used to introduce nonlinearities:."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb450cc-dc58-4697-944f-a1d3cead6791",
   "metadata": {},
   "source": [
    "### The sigmoid\n",
    "$$f(x) = \\frac{1}{1+\\exp(-x)}  $$\n",
    "So the output of the sigmoid ranges from 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "id": "93b78874-1400-4099-b634-1ee51f7c7c92",
   "metadata": {},
   "source": [
    "x = np.array(range(-1000,1000))/100\n",
    "y = 1/(1+np.exp(-x))\n",
    "plt.plot(x,y)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3c511f86-2338-452e-8613-11eae07bc7b3",
   "metadata": {},
   "source": [
    "### The tanh function \n",
    "They use a similar kind of S-shaped nonlinearity, but instead of ranging \n",
    "from 0 to 1, the output of tanh neurons ranges from −1 to 1\n",
    "$$f(x) = \\tanh(x)$$."
   ]
  },
  {
   "cell_type": "code",
   "id": "a8fa0620-56ad-4595-873a-16ce1e914973",
   "metadata": {},
   "source": [
    "z = np.tanh(x)\n",
    "plt.plot(x,z)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "83febd70-d24c-4075-b12e-ceb0eeb56194",
   "metadata": {},
   "source": [
    "### Rectified Linear Unit (ReLU) \n",
    "It uses the function \n",
    "$$f(x) = \\max(0,x)$$\n",
    "resulting in a characteristic hockey-stick-shaped response. The ReLU has recently become popular for many tasks, especially in computer vision."
   ]
  },
  {
   "cell_type": "code",
   "id": "c549c8ea-0ba2-40b6-bd68-15171db7b90b",
   "metadata": {},
   "source": [
    "t = np.array([x,np.zeros(len(x))]).max(axis=0)\n",
    "plt.plot(x,t)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3ab65f7d-25fa-4614-b89f-02e52ad0b66e",
   "metadata": {},
   "source": [
    "### Softmax Output Layers\n",
    "Sometimes  we want our output vector to be a probability distribution over a set of mutually exclusive labels. As a result, the desired output vector is of the form $p=[p_1,p_2,\\ldots,p_n]$, where $\\sum_{i=1}^n p_i = 1$. This can be achieved by a *softmax layer*.  Unlike in other kinds of layers, the output of a neuron in a softmax layer depends on the outputs of all the other neurons in its layer. This is because we require the sum of all the outputs to be equal to 1. Letting $x_i$ be the logit of the $i$tℎ softmax neuron, we can achieve this normalization by setting its output to\n",
    "$$y_i=\\frac{\\exp(x_i)}{\\sum_{i=1}^n \\exp(x_i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2787bbf4-132b-4e52-b85d-1ca18fb323ec",
   "metadata": {},
   "source": [
    "## Training\n",
    "Initially, weight matrices are filled with small random values. What comes next is to gradually adjust these weights, based on a feedback signal. This gradual adjustment, also called *training*, is the learning that machine learning is all about. This happens within a *training loop*, which works as follows. Repeat these steps in a loop, until the loss seems sufficiently low:\n",
    "- Draw a batch of training samples, x, and corresponding targets, y_true.\n",
    "- Run the model on x (a step called the forward pass) to obtain predictions, y_pred.\n",
    "- Compute the loss of the model on the batch, a measure of the mismatch between y_pred and y_true.\n",
    "- Update all weights of the model in a way that slightly reduces the loss on this batch.\n",
    "\n",
    "The most difficult is step 4. One naive solution would be to freeze all weights in the model except the one scalar coefficient being considered, and try different values for this coefficient. But such an approach would be horribly inefficient, because you’d need to compute two forward passes for every individual coefficient. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a399046-8db7-4d75-a5f8-1894c9b1abd2",
   "metadata": {},
   "source": [
    "## Example\n",
    "We create a model consisting of a chain of two dense layers:\n",
    "- the first with $n$ parameters (weigths) and the `relu` activation function;\n",
    "- the second with $m$ weigths and the `softmax` activation function:\n",
    "\n",
    "`model = keras.Sequential([ layers.Dense(n, activation=\"relu\")   layers.Dense(m, activation=\"softmax\")])`\n",
    "\n",
    "Next, we compile the model using the `sparse_categorical_crossentropy` loss function, the `accuracy` metrics, and the `rmsprop` optimizer:\n",
    "\n",
    "`model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864702e4-a088-4f60-a4d9-9fd88f5fea23",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "Gradient descent is much better technique.  All of the functions used in deep learning models transform their input in a smooth and continuous way. Mathematically, these functions are differentiable. If you chain together such functions, the bigger function you obtain is still differentiable. This enables us to use the gradient to describe how the loss varies as you move the model’s coefficients in different directions. If we compute this gradient, we can use it to move the coefficients (all at once in a single update, rather than one at a time) in a direction that decreases the loss. \n",
    "The *mini-batch stochastic gradient descent algorithm (mini-batch SGD)*:\n",
    "- Draw a batch of training samples, x, and corresponding targets, y_true.\n",
    "- Run the model on x to obtain predictions, y_pred (this is called the forward pass).\n",
    "- Compute the loss of the model on the batch, a measure of the mismatch between y_pred and y_true.\n",
    "- Compute the gradient of the loss with regard to the model’s parameters (this is called the backward pass).\n",
    "- Move the parameters a little in the opposite direction from the gradient—for example, W -= learning_rate * gradient—thus reducing the loss on the batch a bit. The learning rate (learning_rate here) would be a scalar factor modulating the “speed” of the gradient descent process.\n",
    "\n",
    "<br/><img src=\"10.png\"/><br/>\n",
    "It’s important to pick a reasonable value for the learning_ rate factor. If it’s too small, the descent down the curve will take many iterations, and it could get stuck in a local minimum. If learning_rate is too large, your updates may end up taking you to completely random locations on the curve.\n",
    "\n",
    "A variant of the mini-batch SGD algorithm would be to draw a single sample and target at each iteration, rather than drawing a batch of data. This would be *true SGD* (as opposed to mini-batch SGD). Alternatively, going to the opposite extreme, we could run every step on all data available, which is called *batch gradient descent*. Each update would then be more accurate, but far more expensive. The efficient compromise between these two extremes is to use mini-batches of reasonable size.\n",
    "\n",
    "There exist many variants of SGD that differ by taking into account previous weight updates when computing the next weight update, rather than just looking at the current value of the gradients:\n",
    "- SGD with momentum, which draws inspiration from physics: If a ball rolling down has enough momentum, the ball won’t get stuck in a ravine and will end up at the global minimum. Momentum is implemented by moving the ball at each step based not only on the current slope value (current acceleration) but also on the current velocity (resulting from past acceleration). Momentum addresses two issues with SGD: convergence speed and local minima. \n",
    "<img src=14.png>\n",
    "- Adagrad\n",
    "- RMSprop\n",
    "\n",
    "All variants of SGD algorithm belong to *optimization* methods or *optimizers*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974f5647",
   "metadata": {},
   "source": [
    "In SGD with momentum the parameter $w$ is updated based not only on the current gradient value but also on the previous parameter update:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b059a8f0",
   "metadata": {},
   "source": [
    "```python\n",
    "past_velocity = 0. \n",
    "momentum = 0.1                \n",
    "while loss > 0.01:            \n",
    "    w, loss, gradient = get_current_parameters()\n",
    "    velocity = past_velocity * momentum - learning_rate * gradient\n",
    "    w = w + momentum * velocity - learning_rate * gradient\n",
    "    past_velocity = velocity\n",
    "    update_parameter(w)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001e0a41-4f86-4f9a-a567-48b8bbfc0986",
   "metadata": {},
   "source": [
    "## The Backpropagation algorithm\n",
    "**Backpropagation** is a way to use the derivatives of simple operations (such as addition, relu, or tensor product) to easily compute the gradient of arbitrarily complex combinations of these atomic operations based on the *chain rule*:\n",
    "$$f(g(x))' = f'(g(x))\\cdot g'(x)$$\n",
    "\n",
    "Backpropagation is the application of the chain rule to a computation graph.\n",
    "<br/><img src=\"11.png\"/><br/>\n",
    "\n",
    "Computation graphs have been an extremely successful abstraction in computer science because they enable us to treat computation as data: a computable expression is encoded as a machine-readable data structure that can be used as the input or output of another program. \n",
    "\n",
    "<br/><img src=\"12.png\"/><br/>\n",
    "<br/><img src=\"13.png\"/><br/>\n",
    "\n",
    "By applying the chain rule to our graph, we obtain the following gradients:\n",
    "- grad(loss_val, w) = 1 * 1 * 2 = 2\n",
    "- grad(loss_val, b) = 1 * 1 = 1\n",
    "\n",
    "The *GradientTape* is the API that leverages TensorFlow’s automatic differentiation capabilities. "
   ]
  },
  {
   "cell_type": "code",
   "id": "359ae83f-d05e-4679-aa20-a1fb196e3e37",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import tensorflow as tf\n",
    "x = tf.Variable(0.)                      \n",
    "with tf.GradientTape() as tape:          \n",
    "    y = 2 * x + 3                        \n",
    "grad_of_y_wrt_x = tape.gradient(y, x)  \n",
    "grad_of_y_wrt_x"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "38432ccf-efef-4e5c-b87b-8d8fdb047290",
   "metadata": {},
   "source": [
    "x = tf.Variable(tf.random.uniform((2, 2)))     \n",
    "with tf.GradientTape() as tape:\n",
    "    y = 2 * x + 3 \n",
    "grad_of_y_wrt_x = tape.gradient(y, x)    \n",
    "grad_of_y_wrt_x"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c600ad1d-054b-4627-a8d3-d09b277d3a43",
   "metadata": {},
   "source": [
    "W = tf.Variable(tf.random.uniform((2, 2)))\n",
    "b = tf.Variable(tf.zeros((2,)))\n",
    "x = tf.random.uniform((2, 2)) \n",
    "with tf.GradientTape() as tape:\n",
    "    y = tf.matmul(x, W) + b                         \n",
    "grad_of_y_wrt_W_and_b = tape.gradient(y, [W, b])    \n",
    "x, grad_of_y_wrt_W_and_b"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0dcd571c-23ef-4513-859e-4dbe91539b53",
   "metadata": {},
   "source": [
    "## Example\n",
    "Next, the model starts to iterate on the training data in mini-batches of $s$ samples, $I$ times over (each iteration over all the training data is called an *epoch*). For each batch, the model will compute the gradient of the loss with regard to the weights using the Backpropagation algorithm, and move the weights in the direction that will reduce the value of the loss for this batch:\n",
    "\n",
    "`model.fit(train_images, train_labels, epochs=I, batch_size=s)`\n",
    "\n",
    "After training we can use the model to make predictions:\n",
    "\n",
    "`predictions = model.predict(test_digits)`\n",
    "\n",
    "and evaluate the model on new data:\n",
    "\n",
    "`test_loss, test_acc = model.evaluate(test_images, test_labels)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0b03c5-f005-4a59-bc40-b8fa3caf94b1",
   "metadata": {},
   "source": [
    "# Laboratory 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cdb77f-ec4c-4b3a-9de0-aeecc92392c7",
   "metadata": {},
   "source": [
    "## Task 1. \n",
    "Import necessary libraries: `from tensorflow import keras` and  `\n",
    "from tensorflow.keras import layers.\n",
    "\n",
    "`\n",
    "Load `train_images`, `train_labels`, `test_images`, `test_labels` from data set `mnist` from `tensorflow.keras.datasets`. What are the shape and type of `train_images` and `test_images`? \n",
    "\n",
    "Preprocess the data by reshaping it into the shape the model expects and scaling it so that all values are in the [0, 1] interval: \n",
    "Change tensors `train_images` and `test_images` into rank-2 tensors of shape $(60000, 28*28)$, $(10000, 28*28)$, respectively. Change type of these tensors into `float32` and standardize them: divide them by the their maximum value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fff4bd0-6c11-460e-9a98-b25a4635121c",
   "metadata": {},
   "source": [
    "## Taks 2\n",
    "Using function `Sequential` from `Keras` create a model consisting of a sequence of two densely connected neural layers. The first layer should consists of 512 parameters and its activation function should be reLU. The second (and last) layer is to be a 10-way softmax classification layer, which means it will return an array of 10 probability scores (summing to 1). Each score will be the probability that the current digit image belongs to one of 10 digit classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5013893d-1ff4-4dd4-b354-8142afd3a681",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "Compile the model using `rmsprop` algorithm as an optimizer, `sparse_categorical_crossentropy` as a loss function, and the `accuracy` (the fraction of the images that were correctly classified) as a metrics. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846e1388-b483-4879-abc2-d9ac59566338",
   "metadata": {},
   "source": [
    "## Task 4\n",
    "Fit the model to its training data using 5 epochs and batch_size=128."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8178b8cd-b1c5-4a30-945c-3425e0427921",
   "metadata": {},
   "source": [
    "## Task 5\n",
    "Use the trained model to predict class probabilities for digits from `test_images` and for each digit compare the index with the greatest probability with apropriate element in `test_labels`. Find the first digit from  `test_images` for which label predicted by the model is different from the true label form `test_labels`. What is the true digit and its prediction? Plot this digit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88de586a-99d1-4800-b5fa-e3b88f75966a",
   "metadata": {},
   "source": [
    "## Task 6\n",
    "Evaluate the model on training and test data. Compare the test-set accuracy/loss with the training-set accuracy/loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5954fa-41d3-471c-a935-de86f50b7dfd",
   "metadata": {},
   "source": [
    "## Task 7\n",
    "Do tasks 4 and 6 using:\n",
    "- 100 epochs and batch_size=len(train_labels)\n",
    "- 1 epoch and batch_size=1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4865550f-077c-435d-afe5-fff74d5c5bf1",
   "metadata": {},
   "source": [
    "## Task 8\n",
    "Create a model consisting of a sequence of three densely connected neural layers. The first two layers should consists of 256 parameters and their activation function should be reLU. The last layer is to be a 10-way softmax classification layer. Next, compile the model in the same way as in Task 3 and fit the model to its training data using 10 epochs and batch_size=64. Finally, evaluate the model on training and test data and compare the training-set/test-set accuracy with the training-set/test-set accuracy of the model from Task 2. What model has better accuracy for training-set and test-set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677ffdfb-f03b-424f-a1fc-6db118e88d23",
   "metadata": {},
   "source": [
    "## Task 9\n",
    "Check, whether further increasing the number of layers influences the accuracy of a deep learning model. Experiment with various numbers of parameters, batch sizes, and numbers of epochs. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
